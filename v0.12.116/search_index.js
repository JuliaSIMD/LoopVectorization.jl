var documenterSearchIndex = {"docs":
[{"location":"examples/array_interface/#Array-Interface","page":"Array Interface","title":"Array Interface","text":"","category":"section"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"LoopVectorization uses ArrayInterface.jl to describe the memory layout of arrays. By supporting the interface, LoopVectorization will be able to support compatible AbstractArray types. StaticArrays.jl and HybridArrays.jl are two example libraries providing array types supporting the interface.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"StaticArrays.SArray itself is not compatible, because LoopVectorization needs access to a pointer. However,StaticArrays.MArrays are compatible. Loops featuring StaticArrays.StaticArray will result in a fall-back loop being executed that wasn't optimized by LoopVectorization, but instead simply had @inbounds @fastmath applied to the loop. This can often still yield reasonable to good performance, saving you from having to write more than one version of the loop to get good performance and correct behavior just because the array types happen to be different.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"By supporting the interface, using LoopVectorization can simplify implementing many operations like matrix multiply while still getting good performance. For example, instead of a few hundred lines of code to define matix multiplication in StaticArrays, one could simply write:","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"using StaticArrays, LoopVectorization\n\n@inline function AmulB!(C, A, B)\n    @turbo for n ∈ axes(C,2), m ∈ axes(C,1)\n        Cmn = zero(eltype(C))\n        for k ∈ axes(B,1)\n            Cmn += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\n    C\nend\n@inline AmulB(A::MMatrix{M,K,T}, B::MMatrix{K,N,T}) where {M,K,N,T} = AmulB!(MMatrix{M,N,T}(undef), A, B)\n@inline AmulB(A::SMatrix, B::SMatrix) = SMatrix(AmulB(MMatrix(A), MMatrix(B)))","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"Through converting back and fourth between SMatrix and MMatrix, we can still use LoopVectorization to implement SMatrix multiplication, and in most cases get better performance than the unrolled methods from the library. Unfortunately, it is still suboptimal because the compiler isn't able to elide the copying, but the temporaries are all stack-allocated, making the code allocateion free. We can benchmark our simple implementation vs the StaticArrays.SMatrix and StaticArrays.MMatrix methods:","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"using BenchmarkTools, LinearAlgebra, DataFrames, VegaLite\nBLAS.set_num_threads(1);\n\nmatdims(x::Integer) = (x, x, x)\nmatdims(x::NTuple{3}) = x\nmatflop(x::Integer) = 2x^3\nmatflop(x::NTuple{3}) = 2prod(x)\n\nfunction runbenches(sr, ::Type{T}, fa = identity, fb = identity) where {T}\n    bench_results = Matrix{Float64}(undef, length(sr), 4);\n    for (i,s) ∈ enumerate(sr)\n        M, K, N = matdims(s)\n        Am = @MMatrix rand(T, M, K)\n        Bm = @MMatrix rand(T, K, N)\n        As = Ref(SMatrix(Am));\n        Bs = Ref(SMatrix(Bm));\n        Css = fa(As[]) * fb(Bs[]);\n        Csl = AmulB(fa(As[]), fb(Bs[]))\n        Cms = similar(Css); mul!(Cms, fa(Am), fb(Bm));\n        Cml = similar(Css); AmulB!(Cml, fa(Am), fb(Bm));\n        @assert Array(Css) ≈ Array(Csl) ≈ Array(Cms) ≈ Array(Cml) # Once upon a time Julia crashed on ≈ for large static arrays\n        bench_results[i,1] = @belapsed $fa($As[]) * $fb($Bs[])\n        bench_results[i,2] = @belapsed AmulB($fa($As[]), $fb($Bs[]))\n        bench_results[i,3] = @belapsed mul!($Cms, $fa($Am), $fb($Bm))\n        bench_results[i,4] = @belapsed AmulB!($Cml, $fa($Am), $fb($Bm))\n        @show s, bench_results[i,:]\n    end\n    gflops = @. 1e-9 * matflop(sr) / bench_results\n    array_type = append!(fill(\"Static\", 2length(sr)), fill(\"Mutable\", 2length(sr)))\n    sa = fill(\"StaticArrays\", length(sr)); lv = fill(\"LoopVectorization\", length(sr));\n    matmul_lib = vcat(sa, lv, sa, lv);\n    sizes = reduce(vcat, (sr for _ ∈ 1:4))\n    DataFrame(\n        Size = sizes, Time = vec(bench_results), GFLOPS = vec(gflops),\n        ArrayType = array_type, MatmulLib = matmul_lib, MulType = array_type .* ' ' .* matmul_lib\n    )\nend\n\ndf = runbenches(1:24, Float64);\ndf |> @vlplot(:line, x = :Size, y = :GFLOPS, color = :MulType, height=640,width=960) |> save(\"sarraymatmul.svg\")","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"This yields: (Image: sarray_benchmarks) Our AmulB! for MMatrixes was the fastest at all sizes except 2x2, where it lost out to AmulB for SMatrix, which in turn was faster than the hundreds of lines of StaticArrays code at all sizes except 3x3,  5x5, and  6x6.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"Additionally, HybridArrays.jl can be used when we have a mix of dynamic and statically sized arrays. Maybe we want to multiply two matrices, where each element is a 3x3 matrix:","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"using HybridArrays, StaticArrays, LoopVectorization, BenchmarkTools\n\nA_static = [@SMatrix(rand(3,3)) for i in 1:32, j in 1:32];\nB_static = [@SMatrix(rand(3,3)) for i in 1:32, j in 1:32];\nC_static = similar(A_static);\n\nA_hybrid = HybridArray{Tuple{StaticArrays.Dynamic(),StaticArrays.Dynamic(),3,3}}(permutedims(reshape(reinterpret(Float64, A_static), (3,3,size(A_static)...)), (3,4,1,2)));\nB_hybrid = HybridArray{Tuple{StaticArrays.Dynamic(),StaticArrays.Dynamic(),3,3}}(permutedims(reshape(reinterpret(Float64, B_static), (3,3,size(B_static)...)), (3,4,1,2)));\nC_hybrid = HybridArray{Tuple{StaticArrays.Dynamic(),StaticArrays.Dynamic(),3,3}}(permutedims(reshape(reinterpret(Float64, C_static), (3,3,size(C_static)...)), (3,4,1,2)));\n\n# C is M x N x I x J\n# A is M x K x I x L\n# B is K x N x L x J\nfunction bmul!(C, A, B)\n    @turbo for n in axes(C,2), m in axes(C,1), j in axes(C,4), i in axes(C,3)\n        Cmnji = zero(eltype(C))\n        for k in axes(B,1), l in axes(B,3)\n            Cmnji += A[m,k,i,l] * B[k,n,l,j]\n        end\n        C[m,n,i,j] = Cmnji\n    end\nend","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"This yields","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"julia> @benchmark bmul!($C_hybrid, $A_hybrid, $B_hybrid)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     15.550 μs (0.00% GC)\n  median time:      15.663 μs (0.00% GC)\n  mean time:        15.685 μs (0.00% GC)\n  maximum time:     50.286 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n  \njulia> @benchmark mul!($C_static, $A_static, $B_static)\nBenchmarkTools.Trial:\n  memory estimate:  336 bytes\n  allocs estimate:  6\n  --------------\n  minimum time:     277.736 μs (0.00% GC)\n  median time:      278.035 μs (0.00% GC)\n  mean time:        278.310 μs (0.00% GC)\n  maximum time:     299.259 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> all(I -> C_hybrid[Tuple(I)[1],Tuple(I)[2],:,:] ≈ C_static[I], CartesianIndices(C_static))\ntrue\n\njulia> length(C_hybrid) * size(B_hybrid,1) * size(B_hybrid,3) * 2e-9 / 15.55e-6 # GFLOPS loops + hybrid arrays\n113.79241157556271\n\njulia> length(C_hybrid) * size(B_hybrid,1) * size(B_hybrid,3) * 2e-9 / 277.736e-6 # GFLOPS LinearAlgebra.mul! + StaticArrays\n6.371057407034018","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"When using LoopVectorization + HybridArrays, you may often find that you often get the best performance when the leading dimensions are either an even multiple of 8, or relatively large. This will often mean not leading with a small static dimension, which is commonly best practice when not using LoopVectorization.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"If you happen to like tensor operations such as from this last example, you're also strongly encouraged to check out Tullio.jl which provides index-notation that is both much more convenient and much less error-prone than writing out loops, and uses both LoopVectorization (if you using LoopVectorization before @tullio) as well as multiple threads to maximize performance.","category":"page"},{"location":"devdocs/constructing_loopsets/#Constructing-LoopSets","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/#Loop-expressions","page":"Constructing LoopSets","title":"Loop expressions","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When applying @turbo to a loop expression, it creates a LoopSet without awareness to type information, and then condenses the information into a summary which is passed as type information to a generated function.","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"julia> @macroexpand @turbo for m ∈ 1:M, n ∈ 1:N\n           C[m,n] = zero(eltype(B))\n           for k ∈ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end\nquote\n    var\"##vptr##_C\" = LoopVectorization.stridedpointer(C)\n    var\"##vptr##_A\" = LoopVectorization.stridedpointer(A)\n    var\"##vptr##_B\" = LoopVectorization.stridedpointer(B)\n    begin\n        $(Expr(:gc_preserve, :(LoopVectorization._turbo_!(Val{(0, 0)}(), Tuple{:numericconstant, Symbol(\"##zero#270\"), LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.constant, 0x00, 0x01), :LoopVectorization, :setindex!, LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000000, 0x0000000000000007, LoopVectorization.memstore, 0x01, 0x02), :LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000013, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x02, 0x03), :LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000032, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x03, 0x04), :numericconstant, Symbol(\"##reductzero#274\"), LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000003, 0x0000000000000000, LoopVectorization.constant, 0x00, 0x05), :LoopVectorization, :vfmadd_fast, LoopVectorization.OperationStruct(0x0000000000000132, 0x0000000000000003, 0x0000000000000000, 0x0000000000030405, LoopVectorization.compute, 0x00, 0x05), :LoopVectorization, :reduce_to_add, LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000003, 0x0000000000000000, 0x0000000000000601, LoopVectorization.compute, 0x00, 0x01)}, Tuple{LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffe03b), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000103, 0xffffffffffffffd6), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000302, 0xffffffffffffe056), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffffd6)}, Tuple{0, Tuple{}, Tuple{}, Tuple{}, Tuple{}, Tuple{(1, LoopVectorization.IntOrFloat), (5, LoopVectorization.IntOrFloat)}, Tuple{}}, (LoopVectorization.StaticLowerUnitRange{0}(M), LoopVectorization.StaticLowerUnitRange{0}(N), LoopVectorization.StaticLowerUnitRange{0}(K)), var\"##vptr##_C\", var\"##vptr##_A\", var\"##vptr##_B\", var\"##vptr##_C\")), :C, :A, :B))\n    end\nend","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When the corresponding method gets compiled for specific type of A, B, and C, the call to the @generated function _turbo_! get compiled. This causes the summary to be reconstructed using the available type information. This type information can be used, for example, to realize an array has been transposed, and thus correctly identify which axis contains contiguous elements that are efficient to load from. This kind of information cannot be extracted from the raw expression, which is why these decisions are made when the method gets compiled for specific types via the @generated function _turbo_!.","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"The three chief components of the summaries are the definitions of operations, e.g.:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":":LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000013, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x02, 0x03)","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"the referenced array objects:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffe03b)","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"and the set of loop bounds:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"(LoopVectorization.StaticLowerUnitRange{0}(M), LoopVectorization.StaticLowerUnitRange{0}(N), LoopVectorization.StaticLowerUnitRange{0}(K))","category":"page"},{"location":"devdocs/constructing_loopsets/#Broadcasting","page":"Constructing LoopSets","title":"Broadcasting","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When applying the @turbo macro to a broadcast expression, there are no explicit loops, and even the dimensionality of the operation is unknown.  Consequently the LoopSet object must be constructed at compile time. The function and involved operations are their relationships are straightforward to infer from the structure of nested broadcasts:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"julia> Meta.@lower @. f(g(a,b) + c) / d\n:($(Expr(:thunk, CodeInfo(\n    @ none within `top-level scope'\n1 ─ %1 = Base.broadcasted(g, a, b)\n│   %2 = Base.broadcasted(+, %1, c)\n│   %3 = Base.broadcasted(f, %2)\n│   %4 = Base.broadcasted(/, %3, d)\n│   %5 = Base.materialize(%4)\n└──      return %5\n))))\n\njulia> @macroexpand @turbo @. f(g(a,b) + c) / d\nquote\n    var\"##262\" = Base.broadcasted(g, a, b)\n    var\"##263\" = Base.broadcasted(+, var\"##262\", c)\n    var\"##264\" = Base.broadcasted(f, var\"##263\")\n    var\"##265\" = Base.broadcasted(/, var\"##264\", d)\n    var\"##266\" = LoopVectorization.vmaterialize(var\"##265\", Val{:Main}())\nend","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"These nested broadcasted objects already express information very similar to what the LoopSet objects hold. The dimensionality of the objects provides the information on the associated loop dependencies, but again this information is available only when the method is compiled for specific types. The @generated function vmaterialize constructs the LoopSet by recursively evaluating add_broadcast! on all the fields.","category":"page"},{"location":"examples/matrix_vector_ops/#Matrix-Vector-Operations","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"","category":"section"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Here I'll discuss a variety of Matrix-vector operations, naturally starting with matrix-vector multiplication.","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"function jgemvavx!(𝐲, 𝐀, 𝐱)\n    @turbo for i ∈ eachindex(𝐲)\n        𝐲i = zero(eltype(𝐲))\n        for j ∈ eachindex(𝐱)\n            𝐲i += 𝐀[i,j] * 𝐱[j]\n        end\n        𝐲[i] = 𝐲i\n    end\nend","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Using a square Size x Size matrix 𝐀, we find the following results. (Image: Amulvb)","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"If 𝐀 is transposed, or equivalently, if we're instead computing x * 𝐀: (Image: Atmulvb)","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Finally, the three-argument dot product y' * 𝐀 * x: (Image: dot3)","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"The performance impact of alignment is dramatic here.","category":"page"},{"location":"devdocs/lowering/#Lowering","page":"Lowering","title":"Lowering","text":"","category":"section"},{"location":"devdocs/lowering/","page":"Lowering","title":"Lowering","text":"The first step to lowering is picking a strategy for lowering the loops. Then a Julia expression is created following that strategy, converting each of the operations into Julia expressions. This task is made simpler via multiple dispatch making the lowering of the components independent of the larger picture. For example, a load will look like","category":"page"},{"location":"devdocs/lowering/","page":"Lowering","title":"Lowering","text":"vload(vptr_A, (i,j,k))","category":"page"},{"location":"devdocs/lowering/","page":"Lowering","title":"Lowering","text":"with the behavior of this load determined by the types of the arguments. Vectorization is expressed by making an index a _MM{W} type, rather than an integer, and operations with it will either produce another _MM{W} when it will still correspond to contiguous loads, or an Vec{W,<:Integer} if the resulting loads will be discontiguous, so that a gather or scatter! will be used. If all indexes are simply integers, then this produces a scalar load or store.","category":"page"},{"location":"examples/datetime_arrays/#Composite-Types:-DateTime-Arrays","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"","category":"section"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Currently, loops over some types are easier to work with than others. Here we show: (1) a sequential loop over Vector{DateTime} that cannot have @turbo applied directly, and (2) a solution that uses the interpreted integer representation of DateTime.","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"This may be applicable if you have a composite type that may be represented with primitive types.","category":"page"},{"location":"examples/datetime_arrays/#Setting-up-the-Problem","page":"Composite Types: DateTime Arrays","title":"Setting up the Problem","text":"","category":"section"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Here's a simple problem involving timestamps:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Problem statement:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Given: a vector of strictly increasing timestamps.\nOutput: a vector of the same length starting at 0.0 and ending at 1.0. Each intermediate element is scaled proportionally to the length of time since the beginning.","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Sample Output:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"using Dates\n\nsample_input = [\n    Dates.DateTime(2021, 5, 5, 10, 0, 0),\n    Dates.DateTime(2021, 5, 5, 10, 5, 15),\n    Dates.DateTime(2021, 5, 6, 10, 0, 0),\n    Dates.DateTime(2021, 5, 6, 10, 5, 15),\n    Dates.DateTime(2021, 5, 7, 10, 0, 20),\n]\n\nexpected_output = [\n    0.0,\n    0.0018227057053581761,\n    0.499942136326814,\n    0.5017648420321722,\n    1.0,\n]","category":"page"},{"location":"examples/datetime_arrays/#First-Attempt:-Sequential-version-of-the-loop","page":"Composite Types: DateTime Arrays","title":"First Attempt: Sequential version of the loop","text":"","category":"section"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"This implementation satisfies the problem statement by iterating over the examples:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"using Dates\n\nfunction scale_timeseries_sequential(data::Vector{Dates.DateTime})\n  out = similar(data, Float64)\n  ϕ = (data[lastindex(data)] - data[1]).value\n\n  @inbounds for i ∈ eachindex(data)\n      out[i] = (data[i] - data[1]).value / ϕ\n  end\n\n  return out\nend","category":"page"},{"location":"examples/datetime_arrays/#Second-Attempt:-Turbo-Loop","page":"Composite Types: DateTime Arrays","title":"Second Attempt: Turbo Loop","text":"","category":"section"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Our Vector{Dates.DateTime} has an integer interpretation which we can take advantage of here. We'll reinterpret our vector as Int, make the needed adjustments, then apply the @turbo macro to our loop:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"using LoopVectorization, Dates\n\nfunction scale_timeseries_turbo(data::Vector{Dates.DateTime})\n\n  # Interpret our DateTime vector as Int\n  tsi = reinterpret(Int, data)\n\n  out = similar(data, Float64)\n\n  # We've interpreted our data as integers, so we no longer need `.value`\n  ϕ = tsi[lastindex(tsi)] - tsi[1]\n\n  @turbo for i ∈ eachindex(tsi)\n      out[i] = (tsi[i] - tsi[1]) / ϕ\n  end\n\n  return out\nend","category":"page"},{"location":"examples/datetime_arrays/#Benchmarks","page":"Composite Types: DateTime Arrays","title":"Benchmarks","text":"","category":"section"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"We'll benchmark with randomly generated data:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"function generate_timestamps(N::Int64)\n    data = Vector{Dates.DateTime}(undef,N)\n    v = DateTime(1990, 1, 1, 0, 0, 0)\n    for i in 1:N\n        v += Second(rand(1:5, 1)[1])\n        data[i] =v\n    end\n    return data\nend","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"Briefly, the benchmark suggests that the mean time for the sequential vs. turbo solution is a ~3x speedup while holding memory requirements constant:","category":"page"},{"location":"examples/datetime_arrays/","page":"Composite Types: DateTime Arrays","title":"Composite Types: DateTime Arrays","text":"julia> using BenchmarkTools\n\njulia> data_100000 = generate_timestamps(100000);\n\njulia> data_200000 = generate_timestamps(200000);\n\njulia> @benchmark scale_timeseries_sequential(data_100000)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  318.864 μs … 967.760 μs  ┊ GC (min … max): 0.00% … 40.41%\n Time  (median):     321.291 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   332.503 μs ±  52.040 μs  ┊ GC (mean ± σ):  1.97% ±  6.98%\n\n  █▆▅▂▂▂▁                                                       ▁\n  █████████▆▆▆▅▅▅▅▅▄▄▄▄▁▁▄▁▁▃▁▁▄▄▃▃▁▃▄▁▃▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▇ █\n  319 μs        Histogram: log(frequency) by time        701 μs <\n\n Memory estimate: 781.33 KiB, allocs estimate: 2.\n\njulia> @benchmark scale_timeseries_turbo(data_100000)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):   71.942 μs … 933.400 μs  ┊ GC (min … max):  0.00% … 71.93%\n Time  (median):      87.926 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   100.082 μs ±  89.095 μs  ┊ GC (mean ± σ):  11.63% ± 11.43%\n\n  ▄█▃▁                                                        ▁ ▁\n  ████▇▄▁▁▁▁▁▁▁▁▃▄▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ █\n  71.9 μs       Histogram: log(frequency) by time        764 μs <\n\n Memory estimate: 781.33 KiB, allocs estimate: 2.\n\njulia> @benchmark scale_timeseries_sequential(data_200000)\nBenchmarkTools.Trial: 7153 samples with 1 evaluation.\n Range (min … max):  637.692 μs …   2.277 ms  ┊ GC (min … max): 0.00% … 65.01%\n Time  (median):     640.729 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   694.282 μs ± 184.965 μs  ┊ GC (mean ± σ):  3.69% ±  8.68%\n\n  █▆▅▃▂▁                                                        ▁\n  ███████▇▅▆▆▄▄▅▁▁▁▁▁▁▆██▇▅▄▄▁▃▁▃▃▄▁▁▁▁▁▁▁▁▁▁▁▇█▇▇▅▅▄▃▄▄▁▁▁▄▄█▇ █\n  638 μs        Histogram: log(frequency) by time       1.71 ms <\n\n Memory estimate: 1.53 MiB, allocs estimate: 2.\n\njulia> @benchmark scale_timeseries_turbo(data_200000)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  159.023 μs …   2.092 ms  ┊ GC (min … max):  0.00% … 50.30%\n Time  (median):     176.559 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   230.513 μs ± 189.542 μs  ┊ GC (mean ± σ):  11.86% ± 12.80%\n\n  █▇▅▄▄▃▂▂▁          ▁▁                                         ▂\n  ██████████▇▅▅▃▁▄▁▃▁██▇▆▄▅▄▄▅▄▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███▇▅▅▅▅▄▅▄▅█▇▇ █\n  159 μs        Histogram: log(frequency) by time       1.22 ms <\n\n Memory estimate: 1.53 MiB, allocs estimate: 2.","category":"page"},{"location":"performance_accuracy/#Performance-accuracy-tradeoff","page":"-","title":"Performance-accuracy tradeoff","text":"","category":"section"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@turbo performs arithmetic transformations and substitutes standard special functions with implementations that lend themselves better to vectorization but sometimes achieve slightly lower accuracy. The following is a discussion from the Julia slack #performance-helpdesk regarding this tradeoff, how it compares to similar transformations by @fastmath, and how to preserve full accuracy while vectorizing. It is included here for the benefit of anyone looking for information about this. PRs welcome to turn this page into a self-contained discussion or FAQ section.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg 2022-03-30 03:06 UTC","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Does @turbo from LoopVectorization, and by extension @tullio, have the same accuracy caveats as @fastmath, as discussed e.g. here: https://discourse.julialang.org/t/fastmath-macro-accuracy/38847/7? Seeing as LoopVectorization's fallback mode is @inbounds @fastmath","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"More often than not, the transforms enabled by @turbo and @fastmath are going to make results more accurate.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I.e., allowing use of multiple accumulators and fma instructions will help. (edited) ","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@fastmath applied globally makes results less accurate on average, because there are some extreme cases, where code was written with IEEE semantics in mind, performing compensated arithmetic/error accumulation and adjustment.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"By allowing reassociating this code, errors can become catastrophic.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"If you did not write code in such a deliberate manner, you won’t encounter the problem when applying it to your own code.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"It is unsafe to apply to other people’s code, unless you happen to know they also did not write the specific sequence of floating point operations in such an intentional way.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Many functions, like exp and sinpi are written in such an intentional way, and thus experience terrible error when starting julia with –math-mode=fast. But @fastmath exp(x) switches exp implementations to a less accurate, slightly faster, version, rather than applying @fastmath to the contents of the regular exp, so it is not dangerous to do.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I think the fear mongering over fastmath comes from languages like C/C++/Fortran, where you apply it either everywhere or nowhere.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Applying it everywhere is dangerous, because then it gets more and more likely that some code or some library was written in such a way with error tracking, that it causes catastrophic problems.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Applying it locally is the correct approach.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"As would be, IMO, allowing at least associative math everywhere, and letting people opt out in the cases where they’re deliberately avoiding it.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@fastmath and @turbo also enable less accurate functions, like less accurate implementations of ^, sin, etc.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"But especially in the case of @turbo, these versions are way faster. Most of these should still be good to 3 ULP (units in last place)","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"does @turbo have the no nan parts of fastmath?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Thanks, this is super helpful!","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"But especially in the case of @turbo, these versions are way faster.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"So @turbo and @fastmath substitute different fast versions of elementary functions? Is this where @turbo makes better use of the AVX instruction set?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"yes","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"regular @fastmath uses slightly faster scalar functions which LLVM may autovectorize, but @fastmath isn't a powerful enough macro to tell when when a function is called in a loop","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"No, @turbo does not have the no nans part.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Unfortunately, no nans propagates in a way that is at high risk of disabling checks for nans, even when only applying it locally, making no nans basically unusable IMO.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@turbo’s speed gain is mostly from the fact it has versions of special functions that are SIMD-able, not from the lower accuracy.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod  2 hours ago","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"You should be able to manually opt into accurate, but still SIMD-able, versions.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Alec","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@Chris Elrod you should crosspost that to discourse for posterity's sake","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"You should be able to manually opt into accurate, but still SIMD-able, versions.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"You mean a user like me can do that currently or are you saying this as a feature request to yourself/Julia/LLVM?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Is this what VectorizationBase.vadd etc. enable?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"it's a feature request. The VectorizationBase.v versions are the fast but slightly inaccurate ones","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"julia> @inline sincos_simd(x) = SLEEFPirates.sincos(x)\nsincos_simd (generic function with 1 method)\n\njulia> function sincos_turbo_accurate!(sc, x)\n           @turbo for i ∈ eachindex(x)\n               sc[i,1], sc[i,2] = sincos_simd(x[i])\n           end\n       end\nsincos_turbo_accurate! (generic function with 1 method)\n\njulia> function sincos_turbo!(sc, x)\n           @turbo for i ∈ eachindex(x)\n               sc[i,1], sc[i,2] = sincos(x[i])\n           end\n       end\nsincos_turbo! (generic function with 1 method)\n\njulia> function sincos!(sc, x)\n           @inbounds for i ∈ eachindex(x)\n               sc[i,1], sc[i,2] = sincos(x[i])\n           end\n       end\nsincos! (generic function with 1 method)\n\njulia> function sincos_fast!(sc, x)\n           @inbounds @fastmath for i ∈ eachindex(x)\n               sc[i,1], sc[i,2] = sincos(x[i])\n           end\n       end\nsincos_fast! (generic function with 1 method)\n\njulia> x = rand(512); sc = similar(x, length(x), 2);\n\njulia> @btime sincos!($sc,$x)\n  3.918 μs (0 allocations: 0 bytes)\n\njulia> @btime sincos_fast!($sc,$x)\n  3.906 μs (0 allocations: 0 bytes)\n\njulia> @btime sincos_turbo!($sc,$x)\n  614.067 ns (0 allocations: 0 bytes)\n\njulia> @btime sincos_turbo_accurate!($sc,$x)\n  1.702 μs (0 allocations: 0 bytes)","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"The rewrite will probably respect regular fastmath, or have a flag for that.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Anyway, if you want to use the more accurate versions of functions like sincos, you can hide SLEEFPirates.sincos from LV behind a function, like in the above example","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Also, have you seen https://github.com/ARM-software/optimized-routines/tree/master/math? It's a really good set of elementary implementations that should vectorize pretty well. Their pow is of special interest (and their log and exp are really good too) I'm in the processes of rewriting the Base versions to use some of their tricks","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"the one downside of them from a vectorizaiton standpoint is they do slightly more math in UInt form, but most of that is unnecessary","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"It’ll be a while until I get that far on the rewrite!","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"The ideal would be that I can take your Julia implementations and analyze the LLVM to figure out how to vectorize them, without needing any separate implementations.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"That is plan A.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"What’s wrong with UInt, that AVX512DQ is needed on x86?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I’ve at least been seeing more rumors of AVX512 support in Zen4.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"yeah, but Intel is mostly killing it. Also AVX2 is really common...","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"What operations, and can they be implemented by splitting into UInt32s? Not ideal, but still a net win if it enables SIMD…","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Intel’s server CPUs still have AVX512","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I’m still optimistic that it’ll return to consumer CPUs. I suspect it was intended for Alder Lake, but some sort of support to make it work with the efficiency cores wasn’t handled yet.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Maybe the successor to gracemont will support AVX512, in the same way Zen1 supported AVX2.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"UInt32s don’t really work well for things like shifts, nor is there any real overflow detection support.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"So I might need some sort of plan B, where I have to substitute ^ and friends for @llvm.pow or @turbo.pow, and then have my own implementations for these.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"At least for a few critical/special interest functions.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Folks, the effort you're putting into helping people get the most out of their hardware with just some quick annotations and no detailed knowledge of simd and avx and special function implementations is hugely appreciated and I want you to give yourselves a pat on the back","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@alecloudenback The thread is a couple years old, so I'm not eager to revive it. Not sure what the best way to save it for posterity is.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"However, @Chris Elrod, your example left me wondering about exactly how @turbo (and @fastmath for that matter) interacts with function barriers and inlining. For example, how should I think about the behavior and performance of the three versions of f! in the following example?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"const s = 1.2\nf(x) = muladd(s, tanh(x), -x)\n@inline f_inlined(x) = muladd(s, tanh(x), -x)\n\nfunction f!(y, x)\n    @turbo for i in eachindex(x)\n        y[i] = f(x[i])\n    end\nend\n\nfunction f_inlined!(y, x)\n    @turbo for i in eachindex(x)\n        y[i] = f_inlined(x[i])\n    end\nend\n\nfunction f_inlined_manual!(y, x)\n    @turbo for i in eachindex(x)\n        y[i] = muladd(s, tanh(x[i]), -x[i])\n    end\nend\n\njulia> x = randn(512); y = similar(x);\n\njulia> @btime f!(y, x)\n  3.153 μs (0 allocations: 0 bytes)\n\njulia> @btime f_inlined!(y, x)\n  2.966 μs (0 allocations: 0 bytes)\n\njulia> @btime f_inlined_manual!(y, x)\n  2.696 μs (0 allocations: 0 bytes)","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@fastmath only works on syntax, replacing functions like + with Base.FastMath.add_fast, so hiding something behind a function will protect it from @fastmath.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Not sure what the best way to save it for posterity is.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I generally appreciate when documentation includes discussions along these lines. In this case that would include both the LoopVectorization docs for everything that pertains to @turbo, as well as the base Julia docs for details about @fastmath, especially the difference between @fastmath exp(x) and exp(x) under --math-mode=fast–-I think I was simply assuming that @fastmath was all about compiler flags and would transform the elementary arithmetic within each special function.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"@turbo is similar, which is why the sincos example worked, with a few extra caveats:","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"it can still SIMD things behind a function, because it also uses the type system for vectorization.\n@turbo's understanding of what code does is based on syntax (with a little help from types), so if you hide a function from it, it will not know what that function does. It currently assumes the function is very expensive, and will not do anything fancy. In the case of tanh, that's close enough so you don't see much of a difference.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Actually, the difference there is probably because there is a separate tanh and tanh_fast.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Inlining can help a bit, because these functions generally involve polynomials with lots of constants. Inlining lets you avoid reloading these constants on every iteration by hoisting the loads out of the loop.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"You're welcome to make a PR to add documentation based on this discussion.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Oscar Smith","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I think --math-mode=fast should be disabled. It's never actually what you want. https://github.com/JuliaLang/julia/pull/41638 was going to do it, but we got distracted and forgot.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"My general takeaway from this discussion is that both @fastmath and @turbo are mostly safe to use on your own code unless a) you're knowingly exploiting IEEE semantics in accumulations (in which case you wouldn't dream of using these macros anyway), or b) you depend on the 1 ULP accuracy of the standard implementation of a special function (do you though?). Does that sound about right? What I'm still not quite clear on is the issue of the no nans flag in @fastmath being \"basically unusable\"–-is it dangerous or just not helpful?","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"julia> function foo(a,b,c)\n           d = @fastmath a + b\n           e = @fastmath b * c\n           f = d + e\n           isnan(f)\n       end\nfoo (generic function with 2 methods)\n\njulia> @code_llvm debuginfo=:none foo(NaN,NaN,NaN)\ndefine i8 @julia_foo_1015(double %0, double %1, double %2) #0 {\ntop:\n  ret i8 0\n}","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Use of @fastmath enables the compiler to prove that down stream results also are not NaN.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I do not like to lose the ability to check for NaNs.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Yet the above function was compiled away to return false, i.e. that it is impossible for f to be NaN.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"I wouldn't mind if @fastmath allowed optimizing just those functions as though they weren't NaN. If the compiler could prove c or b are 0.0, this would allow eliminating the multiplication and then writing f = d.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"But using that some operations are marked nonans to prove that others aren't NaN is going too far IMO.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"That is, I do like being able to optimize particular code in a way that would change the answer in the presence of NaNs, but I do not actually want to promise the compiler that the values are not NaNs, as I'd still like to check for this later.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Checking arguments still works:","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"julia> function foo(a,b,c)\n           d = @fastmath a + b\n           e = @fastmath b * c\n           f = d + e\n           isnan(f) | isnan(b)\n       end\nfoo (generic function with 2 methods)\n\njulia> @code_llvm debuginfo=:none foo(NaN,NaN,NaN)\ndefine i8 @julia_foo_1019(double %0, double %1, double %2) #0 {\ntop:\n  %3 = fcmp uno double %1, 0.000000e+00\n  %4 = zext i1 %3 to i8\n  ret i8 %4\n} ","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"But sometimes it is easier to check results, e.g. if the arguments are hidden inside some other function.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Looks dangerous to me. So another heuristic for when to avoid @fastmath: c) downstream behavior depends on whether the result was inf or nan (there's a similar flag for inf that's also enabled, right?)","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Yes, there is a similar flag for Inf.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"LoopVectorization does not apply that either.","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Daniel Wennberg","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Just one more clarification question if you can be bothered: returning to the function barrier/inlining example, do I understand correctly that:","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"f! can SIMD\nf_inlined! can SIMD and hoist constants within tanh out of the loop\nf_inlined_manual! can SIMD, replace tanh with tanh_fast, and hoist constants within tanh_fast out of the loop","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Chris Elrod","category":"page"},{"location":"performance_accuracy/","page":"-","title":"-","text":"Yes","category":"page"},{"location":"examples/special_functions/#Special-Functions","page":"Special Functions","title":"Special Functions","text":"","category":"section"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"LoopVectorization supports vectorizing many special functions, for example, to calculate the log determinant of a triangular matrix:","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"function logdettriangle(B::Union{LowerTriangular,UpperTriangular})\n    A = parent(B) # using a triangular matrix would fall back to the default loop.\n    ld = zero(eltype(A))\n    @turbo for n ∈ axes(A,1)\n        ld += log(A[n,n])\n    end\n    ld\nend","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"(Image: selfdot)","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"While Intel's proprietary compilers do the best, LoopVectorization performs very well among open source alternatives. A complicating factor to the above benchmark is that in accessing the diagonals, we are not accessing contiguous elements. A benchmark simply exponentiating a vector shows that gcc also has efficient special function vectorization, but that the autovectorizer disagrees with the discontiguous memory acesses:","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"(Image: selfdot)","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"The similar performance between gfortran and LoopVectorization at multiples of 8 is no fluke: on Linux systems with a recent GLIBC, SLEEFPirates.jl – which LoopVectorization depends on to vectorize these special functions – looks for the GNU vector library and uses these functions if available. Otherwise, it will use native Julia implementations that tend to be slower. As the modulus of vector length and vector width (8, on the host system thanks to AVX512) increases, gfortran shows the performance degredation pattern typical of LLVM-vectorized code.","category":"page"},{"location":"devdocs/evaluating_loops/#Determining-the-strategy-for-evaluating-loops","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"","category":"section"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The heart of the optimizatizations performed by LoopVectorization are given in the determinestrategy.jl file utilizing instruction costs specified in costs.jl. Essentially, it estimates the cost of different means of evaluating the loops. It iterates through the different possible loop orders, as well as considering which loops to unroll, and which to vectorize. It will consider unrolling 1 or 2 loops (but it could settle on unrolling by a factor of 1, i.e. not unrolling), and vectorizing 1.","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The cost estimate is based on the costs of individual instructions and the number of times each one needs to be executed for the given strategy. The instruction cost can be broken into several components:","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The scalar latency is the minimum delay, in clock cycles, associated with the instruction. Think of it as the delay from turning on the water to when water starts coming out the hose.\nThe reciprocal throughput is similar to the latency, but it measures the number of cycles per operation when many of the same operation are repeated in sequence.  Continuing our hose analogy, think of it as the inverse of the flow rate at steady-state. It is typically ≤ the scalar latency.\nThe register pressure measures the register consumption by the operation","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Data on individual instructions for specific architectures can be found on Agner Fog's website. Most of the costs used were those for the Skylake-X architecture.","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Examples of how these come into play:","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Vectorizing a loop will result in each instruction evaluating multiple iterations, but the costs of loads and stores will change based on the memory layouts of the accessed arrays.\nUnrolling can help reduce the number of times an operation must be performed, for example if it can allow us to reuse memory multiple times rather than reloading it every time it is needed.\nWhen there is a reduction, such as performing a sum, there is a dependency chain. Each + has to wait for the previous + to finish executing before it can begin, thus execution time is bounded by latency rather than minimum of the throughput of the + and load operations. By unrolling the loop, we can create multiple independent dependency chains.","category":"page"},{"location":"future_work/#Future-Plans","page":"Future Work","title":"Future Plans","text":"","category":"section"},{"location":"future_work/","page":"Future Work","title":"Future Work","text":"Future plans for LoopVectorization:","category":"page"},{"location":"future_work/","page":"Future Work","title":"Future Work","text":"Support triangular iteration spaces.\nIdentify obvious loop-carried dependencies like A[j] and A[j-1].\nBe able to generate optimized kernels from simple loop-based implementations of operations like Cholesky decompositions or solving triangular systems of equations.\nModel memory and CPU-cache to possibly insert extra loops and packing of data when deemed profitable.\nTrack types of individual operations in the loops. Currently, multiple types in loops aren't really handled, so this is a bit brittle at the moment.\nHandle loops where arrays contain non-primitive types (e.g., Complex numbers) well.","category":"page"},{"location":"future_work/","page":"Future Work","title":"Future Work","text":"Contributions are more than welcome, and I would be happy to assist if anyone would like to take a stab at any of these. Otherwise, while LoopVectorization is a core component to much of my work, so that I will continue developing it, I have many other projects that require active development, so it will be a long time before I am able to address these myself.","category":"page"},{"location":"devdocs/overview/#Developer-Overview","page":"Developer Overview","title":"Developer Overview","text":"","category":"section"},{"location":"devdocs/overview/","page":"Developer Overview","title":"Developer Overview","text":"Here I will try to explain how the library works for the curious or any would-be contributors.","category":"page"},{"location":"devdocs/overview/","page":"Developer Overview","title":"Developer Overview","text":"The library uses a LoopSet object to model loops. The key components of the library can be divided into:","category":"page"},{"location":"devdocs/overview/","page":"Developer Overview","title":"Developer Overview","text":"Defining the LoopSet objects.\nConstructing the LoopSet objects.\nDetermining the strategy of how to evaluate loops.\nLowering the loopset object into a Julia Expr following a strategy.","category":"page"},{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Macros","page":"API reference","title":"Macros","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"@turbo\n@tturbo","category":"page"},{"location":"api/#LoopVectorization.@turbo","page":"API reference","title":"LoopVectorization.@turbo","text":"@turbo\n\nAnnotate a for loop, or a set of nested for loops whose bounds are constant across iterations, to optimize the computation. For example:\n\nfunction AmulB!(C, A, B)\n    @turbo for m ∈ indices((A,C), 1), n ∈ indices((B,C), 2) # indices((A,C),1) == axes(A,1) == axes(C,1)\n        Cₘₙ = zero(eltype(C))\n        for k ∈ indices((A,B), (2,1)) # indices((A,B), (2,1)) == axes(A,2) == axes(B,1)\n            Cₘₙ += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cₘₙ\n    end\nend\n\nThe macro models the set of nested loops, and chooses an ordering of the three loops to minimize predicted computation time.\n\nCurrent limitations:\n\nIt assumes that loop iterations are independent.\nIt does not perform bounds checks.\nIt assumes that each loop iterates at least once. (Use @turbo check_empty=true to lift this assumption.)\nThat there is only one loop at each level of the nest.\n\nIt may also apply to broadcasts:\n\njulia> using LoopVectorization\n\njulia> a = rand(100);\n\njulia> b = @turbo exp.(2 .* a);\n\njulia> c = similar(b);\n\njulia> @turbo @. c = exp(2a);\n\njulia> b ≈ c\ntrue\n\nExtended help\n\nAdvanced users can customize the implementation of the @turbo-annotated block using keyword arguments:\n\n@turbo inline=false unroll=2 thread=4 body\n\nwhere body is the code of the block (e.g., for ... end).\n\nthread is either a Boolean, or an integer. The integer's value indicates the number of threads to use. It is clamped to be between 1 and min(Threads.nthreads(),LoopVectorization.num_cores()). false is equivalent to 1, and true is equivalent to min(Threads.nthreads(),LoopVectorization.num_cores()).\n\ninline is a Boolean. When true, body will be directly inlined into the function (via a forced-inlining call to _turbo_!). When false, it wont force inlining of the call to _turbo_! instead, letting Julia's own inlining engine determine whether the call to _turbo_! should be inlined. (Typically, it won't.) Sometimes not inlining can lead to substantially worse code generation, and >40% regressions, even in very large problems (2-d convolutions are a case where this has been observed). One can find some circumstances where inline=true is faster, and other circumstances where inline=false is faster, so the best setting may require experimentation. By default, the macro tries to guess. Currently the algorithm is simple: roughly, if there are more than two dynamically sized loops or and no convolutions, it will probably not force inlining. Otherwise, it probably will.\n\ncheck_empty (default is false) determines whether or not it will check if any of the iterators are empty. If false, you must ensure yourself that they are not empty, else the behavior of the loop is undefined and (like with @inbounds) segmentation faults are likely.\n\nunroll is an integer that specifies the loop unrolling factor, or a tuple (u₁, u₂) = (4, 2) signaling that the generated code should unroll more than one loop. u₁ is the unrolling factor for the first unrolled loop and u₂ for the next (if present), but it applies to the loop ordering and unrolling that will be chosen by LoopVectorization, not the order in body. uᵢ=0 (the default) indicates that LoopVectorization should pick its own value, and uᵢ=-1 disables unrolling for the correspond loop.\n\nThe @turbo macro also checks the array arguments using LoopVectorization.check_args to try and determine if they are compatible with the macro. If check_args returns false, a fall back loop annotated with @inbounds and @fastmath is generated. Note that VectorizationBase provides functions such as vadd and vmul that will ignore @fastmath, preserving IEEE semantics both within @turbo and @fastmath. check_args currently returns false for some wrapper types like LinearAlgebra.UpperTriangular, requiring you to use their parent. Triangular loops aren't yet supported.\n\nSetting the keyword argument warn_check_args=true (e.g. @turbo warn_check_args=true for ...) in a loop or broadcast statement will cause it to warn once if LoopVectorization.check_args fails and the fallback loop is executed instead of the LoopVectorization-optimized loop. Setting it to an integer > 0 will warn that many times, while setting it to a negative integer will warn an unlimited amount of times. The default is warn_check_args = 0.\n\n\n\n\n\n","category":"macro"},{"location":"api/#LoopVectorization.@tturbo","page":"API reference","title":"LoopVectorization.@tturbo","text":"@tturbo\n\nEquivalent to @turbo, except it adds thread=true as the first keyword argument. Note that later arguments take precendence.\n\nMeant for convenience, as @tturbo is shorter than @turbo thread=true.\n\n\n\n\n\n","category":"macro"},{"location":"api/#map-like-constructs","page":"API reference","title":"map-like constructs","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"vmap\nvmap!\nvmapnt\nvmapnt!\nvmapntt\nvmapntt!","category":"page"},{"location":"api/#LoopVectorization.vmap","page":"API reference","title":"LoopVectorization.vmap","text":"vmap(f, a::AbstractArray)\nvmap(f, a::AbstractArray, b::AbstractArray, ...)\n\nSIMD-vectorized map, applying f to each element of a (or paired elements of a, b, ...) and returning a new array.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmap!","page":"API reference","title":"LoopVectorization.vmap!","text":"vmap!(f, destination, a::AbstractArray)\nvmap!(f, destination, a::AbstractArray, b::AbstractArray, ...)\n\nVectorized-map!, applying f to batches of elements of a (or paired batches of a, b, ...) and storing the result in destination.\n\nThe function f must accept VectorizationBase.AbstractSIMD inputs. Ideally, all this requires is making sure that f is defined to be agnostic with respect to input types, but if the function f contains branches or loops, more work will probably be needed. For example, a function\n\nf(x) = x > 0 ? log(x) : inv(x)\n\ncan be rewritten into\n\nusing IfElse\nf(x) = IfElse.ifelse(x > 0, log(x), inv(x))\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapnt","page":"API reference","title":"LoopVectorization.vmapnt","text":"vmapnt(f, a::AbstractArray)\nvmapnt(f, a::AbstractArray, b::AbstractArray, ...)\n\nA \"non-temporal\" variant of vmap. This can improve performance in cases where destination will not be needed soon.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapnt!","page":"API reference","title":"LoopVectorization.vmapnt!","text":"vmapnt!(::Function, dest, args...)\n\nThis is a vectorized map implementation using nontemporal store operations. This means that the write operations to the destination will not go to the CPU's cache. If you will not immediately be reading from these values, this can improve performance because the writes won't pollute your cache. This can especially be the case if your arguments are very long.\n\njulia> using LoopVectorization, BenchmarkTools\njulia> x = rand(10^8); y = rand(10^8); z = similar(x);\njulia> f(x,y) = exp(-0.5abs2(x - y))\nf (generic function with 1 method)\njulia> @benchmark map!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     439.613 ms (0.00% GC)\n  median time:      440.729 ms (0.00% GC)\n  mean time:        440.695 ms (0.00% GC)\n  maximum time:     441.665 ms (0.00% GC)\n  --------------\n  samples:          12\n  evals/sample:     1\njulia> @benchmark vmap!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     178.147 ms (0.00% GC)\n  median time:      178.381 ms (0.00% GC)\n  mean time:        178.430 ms (0.00% GC)\n  maximum time:     179.054 ms (0.00% GC)\n  --------------\n  samples:          29\n  evals/sample:     1\njulia> @benchmark vmapnt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     144.183 ms (0.00% GC)\n  median time:      144.338 ms (0.00% GC)\n  mean time:        144.349 ms (0.00% GC)\n  maximum time:     144.641 ms (0.00% GC)\n  --------------\n  samples:          35\n  evals/sample:     1\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapntt","page":"API reference","title":"LoopVectorization.vmapntt","text":"vmapntt(f, a::AbstractArray)\nvmapntt(f, a::AbstractArray, b::AbstractArray, ...)\n\nA threaded variant of vmapnt.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapntt!","page":"API reference","title":"LoopVectorization.vmapntt!","text":"vmapntt!(::Function, dest, args...)\n\nA threaded variant of vmapnt!.\n\n\n\n\n\n","category":"function"},{"location":"api/#filter-like-constructs","page":"API reference","title":"filter-like constructs","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"vfilter\nLoopVectorization.vfilter!","category":"page"},{"location":"api/#LoopVectorization.vfilter","page":"API reference","title":"LoopVectorization.vfilter","text":"vfilter(f, a::AbstractArray)\n\nSIMD-vectorized filter, returning an array containing the elements of a for which f return true.\n\nThis function requires AVX512 to be faster than Base.filter, as it adds compressstore instructions.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vfilter!","page":"API reference","title":"LoopVectorization.vfilter!","text":"vfilter!(f, a::AbstractArray)\n\nSIMD-vectorized filter!, removing the element of a for which f is false.\n\n\n\n\n\n","category":"function"},{"location":"api/#reduce-like-constructs","page":"API reference","title":"reduce-like constructs","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"vreduce\nvmapreduce","category":"page"},{"location":"api/#LoopVectorization.vreduce","page":"API reference","title":"LoopVectorization.vreduce","text":"vreduce(op, destination, A::DenseArray...)\n\nVectorized version of reduce. Reduces the array A using the operator op.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapreduce","page":"API reference","title":"LoopVectorization.vmapreduce","text":"vmapreduce(f, op, A::DenseArray...)\n\nVectorized version of mapreduce. Applies f to each element of the arrays A, and reduces the result with op.\n\n\n\n\n\n","category":"function"},{"location":"devdocs/reference/#Internals-reference","page":"Internals reference","title":"Internals reference","text":"","category":"section"},{"location":"devdocs/reference/#Operation-types","page":"Internals reference","title":"Operation types","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.OperationType\nLoopVectorization.constant\nLoopVectorization.memload\nLoopVectorization.compute\nLoopVectorization.memstore\nLoopVectorization.loopvalue","category":"page"},{"location":"devdocs/reference/#LoopVectorization.OperationType","page":"Internals reference","title":"LoopVectorization.OperationType","text":"OperationType is an @enum for classifying supported operations that can appear in @turbo blocks. Type LoopVectorization.OperationType to see the different types.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.constant","page":"Internals reference","title":"LoopVectorization.constant","text":"An operation setting a variable to a constant value (e.g., a = 0.0)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.memload","page":"Internals reference","title":"LoopVectorization.memload","text":"An operation setting a variable from a memory location (e.g., a = A[i,j])\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.compute","page":"Internals reference","title":"LoopVectorization.compute","text":"An operation computing a new value from one or more variables (e.g., a = b + c)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.memstore","page":"Internals reference","title":"LoopVectorization.memstore","text":"An operation storing a value to a memory location (e.g., A[i,j] = a)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.loopvalue","page":"Internals reference","title":"LoopVectorization.loopvalue","text":"loopvalue indicates an loop variable (i in for i in ...). These are the \"parents\" of compute operations that involve the loop variables.\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#Operation","page":"Internals reference","title":"Operation","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.Operation","category":"page"},{"location":"devdocs/reference/#LoopVectorization.Operation","page":"Internals reference","title":"LoopVectorization.Operation","text":"Operation\n\nA structure to encode a particular action occuring inside an @turbo block.\n\nFields\n\nidentifier::Int64\nA unique identifier for this operation. identifer(op::Operation) returns the index of this operation within operations(ls::LoopSet).\nvariable::Symbol\nThe name of the variable storing the result of this operation. For a = val this would be :a. For array assignments A[i,j] = val this would be :A.\nelementbytes::Int64\nIntended to be the size of the result, in bytes. Often inaccurate, not to be relied on.\ninstruction::LoopVectorization.Instruction\nThe specific operator, e.g., identity or +\nnode_type::LoopVectorization.OperationType\nThe OperationType associated with this operation\ndependencies::Vector{Symbol}\nThe loop variables this operation depends on\nreduced_deps::Vector{Symbol}\nAdditional loop dependencies that must execute before this operation can be performed successfully (often needed in reductions)\nparents::Vector{LoopVectorization.Operation}\nOperations whose result this operation depends on\nchildren::Vector{LoopVectorization.Operation}\nOperations who depend on this result\nref::LoopVectorization.ArrayReferenceMeta\nFor memload or memstore, encodes the array location\nmangledvariable::Symbol\ngensymmed name of result.\nreduced_children::Vector{Symbol}\nLoop variables that consumers of this operation depend on. Often used in reductions to replicate assignment of initializers when unrolling.\nu₁unrolled::Bool\nCached value for whether u₁loopsym ∈ loopdependencies(op)\nu₂unrolled::Bool\nCached value for whether u₂loopsym ∈ loopdependencies(op)\nvectorized::Bool\nCached value for whether vectorized ∈ loopdependencies(op)\nrejectcurly::Bool\nCached value for whether or not to lower memop using Unrolled\nrejectinterleave::Bool\nCached value for whether or not to lower memop by interleaving it with offset operations\n\nExample\n\njulia> using LoopVectorization\n\njulia> AmulBq = :(for m ∈ 1:M, n ∈ 1:N\n           C[m,n] = zero(eltype(B))\n           for k ∈ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end);\n\njulia> lsAmulB = LoopVectorization.LoopSet(AmulBq);\n\njulia> LoopVectorization.operations(lsAmulB)\n6-element Vector{LoopVectorization.Operation}:\n var\"##RHS#245\" = var\"##zero#246\"\n C[m, n] = var\"##RHS#245\"\n var\"##tempload#248\" = A[m, k]\n var\"##tempload#249\" = B[k, n]\n var\"##RHS#245\" = LoopVectorization.vfmadd(var\"##tempload#248\", var\"##tempload#249\", var\"##RHS#245\")\n var\"##RHS#245\" = LoopVectorization.identity(var\"##RHS#245\")\n\nEach one of these lines is a pretty-printed Operation.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Instructions-and-costs","page":"Internals reference","title":"Instructions and costs","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.Instruction\nLoopVectorization.InstructionCost","category":"page"},{"location":"devdocs/reference/#LoopVectorization.Instruction","page":"Internals reference","title":"LoopVectorization.Instruction","text":"Instruction\n\nInstruction represents a function via its module and symbol. It is similar to a GlobalRef and may someday be replaced by GlobalRef.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.InstructionCost","page":"Internals reference","title":"LoopVectorization.InstructionCost","text":"InstructionCost\n\nStore parameters related to performance for individual CPU instructions.\n\nscaling::Float64\nA flag indicating how instruction cost scales with vector width (128, 256, or 512 bits)\nscalar_reciprocal_throughput::Float64\nThe number of clock cycles per operation when many of the same operation are repeated in sequence. Think of it as the inverse of the flow rate at steady-state. It is typically ≤ the scalar_latency.\nscalar_latency::Int64\nThe minimum delay, in clock cycles, associated with the instruction. Think of it as the delay from turning on a faucet to when water starts coming out the end of the pipe. See also scalar_reciprocal_throughput.\nregister_pressure::Int64\nNumber of floating-point registered used\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Array-references","page":"Internals reference","title":"Array references","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.ArrayReference\nLoopVectorization.ArrayReferenceMeta","category":"page"},{"location":"devdocs/reference/#LoopVectorization.ArrayReference","page":"Internals reference","title":"LoopVectorization.ArrayReference","text":"ArrayReference\n\nA type for encoding an array reference A[i,j] occurring inside an @turbo block.\n\nFields\n\narray::Symbol\nThe array variable\nindices::Vector{Symbol}\nThe list of indices (e.g., [:i, :j]), or name(op) for computed indices.\noffsets::Vector{Int8}\nIndex offset, e.g., a[i+7] would store the 7. offsets is also used to help identify opportunities for avoiding reloads, for example in y[i] = x[i] - x[i-1], the previous load x[i-1] can be \"carried over\" to the next iteration. Only used for small (Int8) offsets.\nstrides::Vector{Int8}\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.ArrayReferenceMeta","page":"Internals reference","title":"LoopVectorization.ArrayReferenceMeta","text":"ArrayReferenceMeta\n\nA type similar to ArrayReference but holding additional information.\n\nFields\n\nref::LoopVectorization.ArrayReference\nThe ArrayReference\nloopedindex::Vector{Bool}\nA vector of Bools indicating whether each index is a loop variable (false for operation-computed indices)\nptr::Symbol\nVariable holding the pointer to the array's underlying storage\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Condensed-types","page":"Internals reference","title":"Condensed types","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"These are used when encoding the @turbo block as a type parameter for passing through to the @generated function.","category":"page"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.ArrayRefStruct\nLoopVectorization.OperationStruct","category":"page"},{"location":"devdocs/reference/#LoopVectorization.ArrayRefStruct","page":"Internals reference","title":"LoopVectorization.ArrayRefStruct","text":"ArrayRefStruct\n\nA condensed representation of an ArrayReference. It supports array-references with up to 8 indexes, where the data for each consecutive index is packed into corresponding 8-bit fields of index_types (storing the enum IndexType), indices (the id for each index symbol), and offsets (currently unused).\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.OperationStruct","page":"Internals reference","title":"LoopVectorization.OperationStruct","text":"OperationStruct\n\nA condensed representation of an Operation.\n\n\n\n\n\n","category":"type"},{"location":"vectorized_convenience_functions/#Convenient-Vectorized-Functions","page":"Vectorized Convenience Functions","title":"Convenient Vectorized Functions","text":"","category":"section"},{"location":"vectorized_convenience_functions/#vmap","page":"Vectorized Convenience Functions","title":"vmap","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"This is simply a vectorized map function.","category":"page"},{"location":"vectorized_convenience_functions/#vmapnt-and-vmapntt","page":"Vectorized Convenience Functions","title":"vmapnt and vmapntt","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"These are like vmap, but use non-temporal (streaming) stores into the destination, to avoid polluting the cache. Likely to yield a performance increase if you wont be reading the values soon.","category":"page"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> f(x,y) = exp(-0.5abs2(x - y))\nf (generic function with 1 method)\n\njulia> x = rand(10^8); y = rand(10^8); z = similar(x);\n\njulia> @benchmark map!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     442.614 ms (0.00% GC)\n  median time:      443.750 ms (0.00% GC)\n  mean time:        443.664 ms (0.00% GC)\n  maximum time:     444.730 ms (0.00% GC)\n  --------------\n  samples:          12\n  evals/sample:     1\n\njulia> @benchmark vmap!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     177.257 ms (0.00% GC)\n  median time:      177.380 ms (0.00% GC)\n  mean time:        177.423 ms (0.00% GC)\n  maximum time:     177.956 ms (0.00% GC)\n  --------------\n  samples:          29\n  evals/sample:     1\n\njulia> @benchmark vmapnt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     143.521 ms (0.00% GC)\n  median time:      143.639 ms (0.00% GC)\n  mean time:        143.645 ms (0.00% GC)\n  maximum time:     143.821 ms (0.00% GC)\n  --------------\n  samples:          35\n  evals/sample:     1\n\njulia> Threads.nthreads()\n36\n\njulia> @benchmark vmapntt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  25.69 KiB\n  allocs estimate:  183\n  --------------\n  minimum time:     30.065 ms (0.00% GC)\n  median time:      30.130 ms (0.00% GC)\n  mean time:        30.146 ms (0.00% GC)\n  maximum time:     31.277 ms (0.00% GC)\n  --------------\n  samples:          166\n  evals/sample:     1","category":"page"},{"location":"vectorized_convenience_functions/#vfilter","page":"Vectorized Convenience Functions","title":"vfilter","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"This function requires LLVM 7 or greater, and is only likely to give better performance if your CPU has AVX512. This is because it uses the compressed store intrinsic, which was added in LLVM 7. AVX512 provides a corresponding instruction, making the operation fast, while other instruction sets must emulate it, and thus are likely to get similar performance with LoopVectorization.vfilter as they do from Base.filter.","category":"page"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(997);\n\njulia> y1 = filter(a -> a > 0.7, x);\n\njulia> y2 = vfilter(a -> a > 0.7, x);\n\njulia> y1 == y2\ntrue\n\njulia> @benchmark filter(a -> a > 0.7, $x)\nBenchmarkTools.Trial:\n  memory estimate:  7.94 KiB\n  allocs estimate:  1\n  --------------\n  minimum time:     955.389 ns (0.00% GC)\n  median time:      1.050 μs (0.00% GC)\n  mean time:        1.191 μs (9.72% GC)\n  maximum time:     82.799 μs (94.92% GC)\n  --------------\n  samples:          10000\n  evals/sample:     18\n\njulia> @benchmark vfilter(a -> a > 0.7, $x)\nBenchmarkTools.Trial:\n  memory estimate:  7.94 KiB\n  allocs estimate:  1\n  --------------\n  minimum time:     477.487 ns (0.00% GC)\n  median time:      575.166 ns (0.00% GC)\n  mean time:        711.526 ns (17.87% GC)\n  maximum time:     9.257 μs (79.17% GC)\n  --------------\n  samples:          10000\n  evals/sample:     193","category":"page"},{"location":"vectorized_convenience_functions/#vmapreduce","page":"Vectorized Convenience Functions","title":"vmapreduce","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"Vectorized version of mapreduce. vmapreduce(f, op, a, b, c) applies f(a[i], b[i], c[i]) for i in eachindex(a,b,c), reducing the results to a scalar with op.","category":"page"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(127); y = rand(127);\n\njulia> @btime vmapreduce(hypot, +, $x, $y)\n  191.420 ns (0 allocations: 0 bytes)\n96.75538300513509\n\njulia> @btime mapreduce(hypot, +, $x, $y)\n  1.777 μs (5 allocations: 1.25 KiB)\n96.75538300513509","category":"page"},{"location":"examples/multithreading/#Multithreading","page":"Multithreading","title":"Multithreading","text":"","category":"section"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization can multithread loops if you pass the argument @turbo thread=true for ... end or equivalently use @tturbo. By default, thread = false, which runs only a single thread. You can also supply a numerical argument to set an upper bound on the number of threads, e.g. @turbo thread=8 for ... end will use up to min(8,Threads.nthreads(),VectorizationBase.num_cores()) threads. VectorizationBase.num_cores() uses Hwloc.jl to get the number of physical cores. Currently, this only works for for loops, but support for broadcasting will come.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Lets look at a few benchmarks.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Taking the first example from the ThreadsX.jl README:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function relative_prime_count(x, N)\n    c = 0\n    @tturbo for i ∈ 1:N\n        c += gcd(x, i) == 1\n    end\n    c\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Benchmarking them:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> @btime ThreadsX.sum(gcd(42, i) == 1 for i in 1:10_000)\n  130.928 μs (3097 allocations: 240.39 KiB)\n2857\n\njulia> @btime relative_prime_count(42, 10_000)\n  3.376 μs (0 allocations: 0 bytes)\n2857","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Note that much of the performance difference here is thanks to SIMD, which requires AVX512 for good performance (trailing_zeros, required by gcd, needs AVX512 for a SIMD version). LoopVectorization is a good choice for loops (a) amenable to SIMD (b) where all arrays are dense and (c) a static schedule would work well. Generally, this means loops built up of relatively primitive arithmetic operations (e.g. +, /, or log), and not, for example, solving differential equations.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"I'll make comparisons with OpenMP through the rest of this, starting with a simple dot product to focus on threading overhead:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function dot_tturbo(a::AbstractArray{T}, b::AbstractArray{T}) where {T <: Real}\n    s = zero(T)\n    @tturbo for i ∈ eachindex(a,b)\n        s += a[i] * b[i]\n    end\n    s\nend\nfunction dotbaseline(a::AbstractArray{T}, b::AbstractArray{T}) where {T}\n    s = zero(T)\n    @fastmath @inbounds @simd for i ∈ eachindex(a,b)\n        s += a[i]' * b[i]\n    end\n    s\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"In C:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"#include<omp.h>\n//  gcc -Ofast -march=native -mprefer-vector-width=512 -fopenmp -shared -fPIC openmp.c -o libomptest.so\ndouble dot(double* a, double* b, long N){\n  double s = 0.0;\n  #pragma omp parallel for reduction(+: s)\n  for(long n = 0; n < N; n++){\n    s += a[n]*b[n];\n  }\n  return s;\n}","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Wrapping it in Julia is straightforward, after compiling:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"using Libdl; const OPENMPTEST = joinpath(pkgdir(LoopVectorization), \"benchmark\", \"libomptest.$(Libdl.dlext)\");\ncdot(a::AbstractVector{Float64},b::AbstractVector{Float64}) = @ccall OPENMPTEST.dot(a::Ref{Float64}, b::Ref{Float64}, length(a)::Clong)::Float64","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Trying out one size to give a perspective on scale:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> N = 10_000; x = rand(N); y = rand(N);\n\njulia> @btime dot($x, $y) # LinearAlgebra\n  1.114 μs (0 allocations: 0 bytes)\n2480.296446711209\n\njulia> @btime dot_turbo($x, $y)\n  761.621 ns (0 allocations: 0 bytes)\n2480.296446711209\n\njulia> @btime dot_tturbo($x, $y)\n  622.723 ns (0 allocations: 0 bytes)\n2480.296446711209\n\njulia> @btime dot_baseline($x, $y)\n  1.294 μs (0 allocations: 0 bytes)\n2480.2964467112097\n\njulia> @btime cdot($x, $y)\n  6.109 μs (0 allocations: 0 bytes)\n2480.2964467112092","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"All these times are fairly fast; wait(Threads.@spawn 1+1) will typically take much longer than even @cdot did here. (Image: realdot)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Now let's look at a more complex example:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function dot_tturbo(ca::AbstractVector{Complex{T}}, cb::AbstractVector{Complex{T}}) where {T}\n    a = reinterpret(reshape, T, ca)\n    b = reinterpret(reshape, T, cb)\n    re = zero(T); im = zero(T)\n    @tturbo for i ∈ axes(a,2) # adjoint(a[i]) * b[i]\n        re += a[1,i] * b[1,i] + a[2,i] * b[2,i]\n        im += a[1,i] * b[2,i] - a[2,i] * b[1,i]\n    end\n    Complex(re, im)\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization currently only supports arrays of type T <: Union{Bool,Base.HWReal}. So to support Complex{T}, we reinterpret the arrays and then write out the corresponding operations. The plan is to eventually have LoopVectorization do this automatically, but for now we require this workaround. Corresponding C:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"void cdot(double* c, double* a, double* b, long N){\n  double r = 0.0, i = 0.0;\n  #pragma omp parallel for reduction(+: r, i)\n  for(long n = 0; n < N; n++){\n    r += a[2*n] * b[2*n  ] + a[2*n+1] * b[2*n+1];\n    i += a[2*n] * b[2*n+1] - a[2*n+1] * b[2*n  ];\n  }\n  c[0] = r;\n  c[1] = i;\n  return;\n}","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"The Julia wrapper:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function cdot(x::AbstractVector{Complex{Float64}}, y::AbstractVector{Complex{Float64}})\n    c = Ref{Complex{Float64}}()\n    @ccall OPENMPTEST.cdot(c::Ref{Complex{Float64}}, x::Ref{Complex{Float64}}, y::Ref{Complex{Float64}}, length(x)::Clong)::Cvoid\n    c[]\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"The complex dot product is more compute bound. Given the same number of elements, we require 2x the memory for complex numbers, 4x the floating point arithmetic, and as we have an array of structs rather than structs of arrays, we need additional instructions to shuffle the data. (Image: complexdot)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"If we take this further to the three-argument dot product, which isn't implemented in BLAS, @tturbo now holds a substantial advantage over the competition:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function dot3(x::AbstractVector{Complex{T}}, A::AbstractMatrix{Complex{T}}, y::AbstractVector{Complex{T}}) where {T}\n    xr = reinterpret(reshape, T, x);\n    yr = reinterpret(reshape, T, y);\n    Ar = reinterpret(reshape, T, A);\n    sre = zero(T)\n    sim = zero(T)\n    @tturbo for n in axes(Ar,3)\n        tre = zero(T)\n        tim = zero(T)\n        for m in axes(Ar,2)\n            tre += xr[1,m] * Ar[1,m,n] + xr[2,m] * Ar[2,m,n]\n            tim += xr[1,m] * Ar[2,m,n] - xr[2,m] * Ar[1,m,n]\n        end\n        sre += tre * yr[1,n] - tim * yr[2,n]\n        sim += tre * yr[2,n] + tim * yr[1,n]\n    end\n    Complex(sre, sim)\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"void cdot3(double* c, double* x, double* A, double* y, long M, long N){\n  double sr = 0.0, si = 0.0;\n#pragma omp parallel for reduction(+: sr, si)\n  for (long n = 0; n < N; n++){\n    double tr = 0.0, ti = 0.0;\n    for(long m = 0; m < M; m++){\n      tr += x[2*m] * A[2*m   + 2*n*N] + x[2*m+1] * A[2*m+1 + 2*n*N];\n      ti += x[2*m] * A[2*m+1 + 2*n*N] - x[2*m+1] * A[2*m   + 2*n*N];\n    }\n    sr += tr * y[2*n  ] - ti * y[2*n+1];\n    si += tr * y[2*n+1] + ti * y[2*n  ];\n  }\n  c[0] = sr;\n  c[1] = si;\n  return;\n}","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"The wrapper is more or less the same as before:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function cdot(x::AbstractVector{Complex{Float64}}, A::AbstractMatrix{Complex{Float64}}, y::AbstractVector{Complex{Float64}})\n    c = Ref{Complex{Float64}}()\n    M, N = size(A)\n    @ccall OPENMPTEST.cdot3(c::Ref{Complex{Float64}}, x::Ref{Complex{Float64}}, A::Ref{Complex{Float64}}, y::Ref{Complex{Float64}}, M::Clong, N::Clong)::Cvoid\n    c[]\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"(Image: complexdot3)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"When testing on my laptop, the C implentation ultimately won, but I will need to investigate further to tell whether this benchmark benefits from hyperthreading, or if it's because LoopVectorization's memory access patterns are less friendly. I plan to work on cache-level blocking to increase memory friendliness eventually, and will likely also allow it to take advantage of hyperthreading/simultaneous multithreading, although I'd prefer a few motivating test problems to look at first. Note that a single core of this CPU is capable of exceeding 100 GFLOPS of double precision compute. The execution units are spending most of their time idle. So the question of whether hypthreading helps may be one of whether or not we are memory-limited.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"For a more compute-limited operation, lets look at matrix multiplication, which requires O(N³) compute for O(N²) memory. Note that it's still easy to be memory-starved in matrix multiplication, especially for larger matrices. While the total memory required may be O(N²), if the memory doesn't fit in the high cache levels, it will have to churn through it. The memory bandwidth requirements are thus O(N³), but cache-level blocking can give it a small enough coefficient that you can make the most of your CPU's theoretical compute. Unlike all the dot product cases (including the 3-argument dot product), which force you to stream most of the memory through the cores. There is no reuse on x and y for the 2-arg dot products, or on memory from A in the the 3-arg dot product.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Here, I compare against other libraries: Intel MKL, OpenBLAS (Julia's default), and two Julia libraries: Tullio.jl and Octavian.jl.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function A_mul_B!(C, A, B)\n    @tturbo for n ∈ indices((C,B), 2), m ∈ indices((C,A), 1)\n        Cmn = zero(eltype(C))\n        for k ∈ indices((A,B), (2,1))\n            Cmn += C[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Benchmarks over the size range 10:5:300: (Image: matmul)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Because LoopVectorization doesn't do cache optimizations yet, MKL, OpenBLAS, and Octavian will all pull ahead for larger matrices. This CPU has a 1 MiB L2 cache per core and 18 cores:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> doubles_per_l2 = (2 ^ 20) ÷ 8\n131072\n\njulia> total_doubles_in_l2 = doubles_per_l2 * (Sys.CPU_THREADS ÷ 2) # doubles_per_l2 * 18\n2359296\n\njulia> doubles_per_mat = total_doubles_in_l2 ÷ 3 # divide up amoung 3 matrices\n786432\n\njulia> sqrt(ans)\n886.8100134752651","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Meaning we could fit three 886x886 matrices in our L2 cache by splitting them up among the cores. The largest matrices benchmarked above, at 300x300, fit comfortably.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Aside from the fact that LoopVectorization did much better than OpenBLAS–Julia's default library–over this size range, LoopVectorization's major advantage that it should perform similarly well for a wide variety of comparable operations and not just GEMM (GEneral Matrix-Matrix multiplication) specifically. GEMM has long been a motivating benchmark, as it's one of the best optimized routines available to compare against and get a sense of how well you're doing vs hand-tuned limits optimized in assembly.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Because it is so well optimized, a standard trick for implementing more general optimized routines is to convert them into GEMM calls. For example, this is commonly done for temsor operations (see, e.g., TensorOperations.jl) as well as for convolutions, e.g. in NNlib's conv_im2col!, their default optimized convolution function.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Lets take a look at convolutions as our next example. We create a batch of a hundred 256x256 images with 3 input channels, and convolve them with a 5x5 kernel producing 6 output channels.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"using NNlib, LoopVectorization, Static\n\nimg = rand(Float32, 260, 260, 3, 100);\nkern = rand(Float32, 5, 5, 3, 6);\nout1 = Array{Float32}(undef, size(img,1)+1-size(kern,1), size(img,2)+1-size(kern,2), size(kern,4), size(img,4));\nout2 = similar(out1);\n\ndcd = NNlib.DenseConvDims(img, kern, flipkernel = true);\n\nfunction kernaxes(::DenseConvDims{2,K,C_in, C_out}) where {K,C_in, C_out} # LoopVectorization can take advantage of static size information\n    K₁ =  StaticInt(1):StaticInt(K[1])\n    K₂ =  StaticInt(1):StaticInt(K[2])\n    Cᵢₙ =  StaticInt(1):StaticInt(C_in)\n    Cₒᵤₜ = StaticInt(1):StaticInt(C_out)\n    (K₁, K₂, Cᵢₙ, Cₒᵤₜ)\nend\n\nfunction convlayer!(out::AbstractArray{<:Any,4}, img, kern, dcd::DenseConvDims)\n    (K₁, K₂, Cᵢₙ, Cₒᵤₜ) = kernaxes(dcd)\n    @tturbo for j₁ ∈ axes(out,1), j₂ ∈ axes(out,2), d ∈ axes(out,4), o ∈ Cₒᵤₜ\n        s = zero(eltype(out))\n        for k₁ ∈ K₁, k₂ ∈ K₂, i ∈ Cᵢₙ\n            s += img[j₁ + k₁ - 1, j₂ + k₂ - 1, i, d] * kern[k₁, k₂, i, o]\n        end\n        out[j₁, j₂, o, d] = s\n    end\n    out\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization likes to take advantage of any static size information when available, so we write kernaxes to extract them from the DenseConvDims object and produce statically sized axes. Otherwise, this code is simply writing the convolutions as a bunch of loops.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"This yields:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> NNlib.conv!(out2, img, kern, dcd);\n\njulia> convlayer!(out1, img, kern, dcd);\n\njulia> out1 ≈ out2\ntrue\n\njulia> @benchmark convlayer!($out1, $img, $kern, $dcd)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     5.377 ms (0.00% GC)\n  median time:      5.432 ms (0.00% GC)\n  mean time:        5.433 ms (0.00% GC)\n  maximum time:     5.682 ms (0.00% GC)\n  --------------\n  samples:          920\n  evals/sample:     1\n\njulia> @benchmark NNlib.conv!($out2, $img, $kern, $dcd)\nBenchmarkTools.Trial:\n  memory estimate:  675.02 MiB\n  allocs estimate:  195\n  --------------\n  minimum time:     182.749 ms (0.00% GC)\n  median time:      190.472 ms (0.60% GC)\n  mean time:        197.527 ms (4.98% GC)\n  maximum time:     300.536 ms (35.82% GC)\n  --------------\n  samples:          26\n  evals/sample:     1","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"By default, the BLAS library called uses multiple threads, but NNlib also threads over the batches using Threads.@threads. This oversubscribes the threads. We thus improve performance by forcing BLAS to use just a single thread, favoring the more granular threading across batches:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> using LinearAlgebra\n\njulia> BLAS.set_num_threads(1)\n\njulia> @benchmark NNlib.conv!($out2, $img, $kern, $dcd)\nBenchmarkTools.Trial:\n  memory estimate:  675.02 MiB\n  allocs estimate:  195\n  --------------\n  minimum time:     124.177 ms (0.00% GC)\n  median time:      128.609 ms (0.93% GC)\n  mean time:        133.574 ms (5.36% GC)\n  maximum time:     235.760 ms (45.17% GC)\n  --------------\n  samples:          38\n  evals/sample:     1","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"This still nets @tturbo a 23x advantage on this machine!","category":"page"},{"location":"examples/multithreading/#FAQ","page":"Multithreading","title":"FAQ","text":"","category":"section"},{"location":"examples/multithreading/#If-I-do-@turbo-threadtrue-for-...-end,-how-many-threads-will-it-use?-Or-if-I-do-@turbo-thread4-for-...-end,-what-then?","page":"Multithreading","title":"If I do @turbo thread=true for ... end, how many threads will it use? Or if I do @turbo thread=4 for ... end, what then?","text":"","category":"section"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization will choose how many threads to use based on the length of the loop ranges and how expensive it estimates evaluating the loop to be. It will at most use one thread per physical core of the system.","category":"page"},{"location":"examples/multithreading/#How-do-I-get-answers-to-my-questions?","page":"Multithreading","title":"How do I get answers to my questions?","text":"","category":"section"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Feel free to ask on Discourse, Zulip, Slack, or GitHub Discussions! I can also add it to the FAQ here, or one in an appropriate section.","category":"page"},{"location":"examples/matrix_multiplication/#Matrix-Multiplication","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"One of the friendliest problems for vectorization is matrix multiplication. Given M × K matrix 𝐀, and K × N matrix 𝐁, multiplying them is like performing M * N dot products of length K. We need M*K + K*N + M*N total memory, but M*K*N multiplications and additions, so there's a lot more arithmetic we can do relative to the memory needed.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"LoopVectorization currently doesn't do any memory-modeling or memory-based optimizations, so it will still run into problems as the size of matrices increases. But at smaller sizes, it's capable of achieving a healthy percent of potential GFLOPS. We can write a single function:","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"function A_mul_B!(C, A, B)\n    @turbo for n ∈ indices((C,B), 2), m ∈ indices((C,A), 1)\n        Cmn = zero(eltype(C))\n        for k ∈ indices((A,B), (2,1))\n            Cmn += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\nend","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"and this can handle all transposed/not-tranposed permutations. LoopVectorization will change loop orders and strategy as appropriate based on the types of the input matrices. For each of the others, I wrote separate functions to handle each case.  Letting all three matrices be square and Size x Size, we attain the following benchmark results:","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"(Image: AmulB) This is classic GEMM, 𝐂 = 𝐀 * 𝐁. GFortran's intrinsic matmul function does fairly well. But all the compilers are well behind LoopVectorization here, which falls behind MKL's gemm beyond 70x70 or so. The problem imposed by alignment is also striking: performance is much higher when the sizes are integer multiplies of 8. Padding arrays so that each column is aligned regardless of the number of rows can thus be very profitable. PaddedMatrices.jl offers just such arrays in Julia. I believe that is also what the -pad compiler flag does when using Intel's compilers.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"(Image: AmulBt) The optimal pattern for 𝐂 = 𝐀 * 𝐁ᵀ is almost identical to that for 𝐂 = 𝐀 * 𝐁. Yet, gfortran's matmul instrinsic stumbles, surprisingly doing much worse than gfortran + loops, and almost certainly worse than allocating memory for 𝐁ᵀ and creating the ecplicit copy.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"ifort did equally well whethor or not 𝐁 was transposed, while LoopVectorization's performance degraded slightly faster as a function of size in the transposed case, because strides between memory accesses are larger when 𝐁 is transposed. But it still performed best of all the compiled loops over this size range, losing out to MKL and eventually OpenBLAS. icc interestingly does better when it is transposed.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"GEMM is easiest when the matrix 𝐀 is not tranposed (assuming column-major memory layouts), because then you can sum up columns of 𝐀 to store into 𝐂. If 𝐀 were transposed, then we cannot efficiently load contiguous elements from 𝐀 that can best stored directly in 𝐂. So for 𝐂 = 𝐀ᵀ * 𝐁, contiguous vectors along the k-loop have to be reduced, adding some overhead. (Image: AtmulB) Packing is critical for performance here. LoopVectorization does not pack, therefore it is well behind MKL and OpenBLAS, which do. Eigen packs, but is poorly optimized for this CPU architecture.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"When both 𝐀 and 𝐁 are transposed, we now have 𝐂 = 𝐀ᵀ * 𝐁ᵀ = (𝐁 * 𝐀)ᵀ. (Image: AtmulBt) Julia, Clang, and gfortran all struggled to vectorize this, because none of the matrices share a contiguous access: M for 𝐂, K for 𝐀ᵀ, and N for 𝐁ᵀ. However, LoopVectorization and all the specialized matrix multiplication functions managed to do about as well as normal; transposing while storing the results takes negligible amounts of time relative to the matrix multiplication itself. The ifort-loop version also did fairly well.","category":"page"},{"location":"examples/sum_of_squared_error/#Sum-of-squared-error","page":"Sum of squared error","title":"Sum of squared error","text":"","category":"section"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"To calculate (y - X * β)'(y - X * β), we can use the following loop.","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"function sse_avx(y, X, β)\n    lp = zero(eltype(y))\n    @turbo for i ∈ eachindex(y)\n        δ = y[i]\n        for j ∈ eachindex(β)\n            δ -= X[i,j] * β[j]\n        end\n        lp += δ * δ\n    end\n    lp\nend","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"This example demonstrates the importance of (not) modeling memory bandwidth and cache, as the performance quickly drops dramatically. However, it still does much better than all the compiled loops, with only the BLAS gemv-based approach matching (and ultimately beating) it in performance, while the other compilers lagged well behind.","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"Performance starts to degrade for sizes larger than 60. Letting N be the size, X was a 3N/2x N/2 matrix. Therefore, performance started to suffer when X had more than about 30 columns (performance is much less sensitive to the number of rows).","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"(Image: sse)","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To install LoopVectorization.jl, simply use the package and ] add LoopVectorization, or","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"LoopVectorization\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Currently LoopVectorization only supports rectangular iteration spaces, although I plan on extending it to triangular and ragged iteration spaces in the future. This means that if you nest multiple loops, the number of iterations of the inner loops shouldn't be a function of the outer loops. For example,","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using LoopVectorization \n\nfunction mvp(P, basis, coeffs::Vector{T}) where {T}\n    C = length(coeffs)\n    A = size(P, 1)\n    p = zero(T)\n    @turbo for c ∈ 1:C\n        pc = coeffs[c]\n        for a = 1:A\n            pc *= P[a, basis[a, c]]\n        end\n        p += pc\n    end\n    p\nend\n\nmaxdeg = 20; nbasis = 1_000; dim = 15;\nr = 1:maxdeg+1\nbasis = rand(r, (dim, nbasis));\ncoeffs = rand(T, nbasis);\nP = rand(T, dim, maxdeg+1);\n\nmvp(P, basis, coeffs)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Aside from loops, LoopVectorization.jl also supports broadcasting.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"danger: Danger\nBroadcasting an Array A when size(A,1) == 1 is NOT SUPPORTED, unless this is known at compile time (e.g., broadcasting a transposed vector is fine). Otherwise, you will probably crash Julia.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> M, K, N = 47, 73, 7;\n\njulia> A = rand(M, K);\n\njulia> b = rand(K);\n\njulia> c = rand(M);\n\njulia> d = rand(1,K,N);\n\njulia> #You can use a LowDimArray when you have a leading dimension of size 1.\n       ldad = LowDimArray{(false,true,true)}(d);\n\njulia> E1 = Array{Float64}(undef, M, K, N);\n\njulia> E2 = similar(E1);\n\njulia> @benchmark @. $E1 = exp($A - $b' +    $d) * $c\nBenchmarkTools.Trial: \n  memory estimate:  112 bytes\n  allocs estimate:  5\n  --------------\n  minimum time:     224.142 μs (0.00% GC)\n  median time:      225.773 μs (0.00% GC)\n  mean time:        229.146 μs (0.00% GC)\n  maximum time:     289.601 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> @benchmark @turbo @. $E2 = exp($A - $b' + $ldad) * $c\nBenchmarkTools.Trial: \n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     19.666 μs (0.00% GC)\n  median time:      19.737 μs (0.00% GC)\n  mean time:        19.759 μs (0.00% GC)\n  maximum time:     29.906 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> E1 ≈ E2\ntrue","category":"page"},{"location":"examples/dot_product/#Dot-Products","page":"Dot Products","title":"Dot Products","text":"","category":"section"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Dot products are simple the sum of the elementwise products of two vectors. They can be interpreted geometrically as (after normalizing by dividing by the norms of both vectors) yielding the cosine of the angle between them. This makes them useful for, for example, the No-U-Turn sampler to check for u-turns (i.e., to check if the current momentum is no longer in the same direction as the change in position).","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"function jdotavx(a, b)\n    s = zero(eltype(a))\n    @turbo for i ∈ eachindex(a, b)\n        s += a[i] * b[i]\n    end\n    s\nend","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"To execute the loop using SIMD (Single Instruction Multiple Data) instructions, you have to unroll the loop. Rather than evaluating the loop as written – adding element-wise products to a single accumulator one after the other – you can multiply short vectors loaded from a and b and add their results to a vector of accumulators. ","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Most modern CPUs found in laptops or desktops have the AVX instruction set, which allows them to operate on 256 bit vectors – meaning the vectors can hold 4 double precision (64 bit) floats. Some have the AVX512 instruction set, which increases the vector size to 512 bits, and also adds many new instructions that make vectorizing easier. To be general across CPUs and data types, I'll refer to the number of elements in the vectors with W. I'll also refer to unrolling a loop by a factor of W and loading vectors from it as \"vectorizing\" that loop.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"In addition to vectorizing the loop, we'll want to unroll it by an additional factor. Given that we have single or double precision floating point elements, most recent CPU cores have a potential throughput of two fused multiply-add (fma) instructions per clock cycle. However, it actually takes about four clock cycles for any of these instructions to execute; a single core is able to work on several in parallel.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"This means that if we used a single vector to accumulate a product, we'd only get to perform one fused multiply add every four clock cycles: we'd have to wait for one instruction to complete before starting the next. By using extra accumulation vectors, we can break up this dependency chain. If we had 8 accumulators, then theoretically we could perform two per clock cycle, and after the 4th cycle, our first operations are done so that we can reuse them.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"However, there is another bottle neck: we can only perform 2 aligned loads per clock cycle (or 1 unaligned load). [Alignment here means with respect to a memory address boundary, if your vectors are 256 bits, then a load/store is aligned if it is with respect to a memory address that is an integer multiple of 32 bytes (256 bits = 32 bytes).] Thus, in 4 clock cycles, we can do up to 8 loads. But each fma requires 2 loads, meaning we are limited to 4 of them per 4 clock cyles, and any unrolling beyond 4 gives us no benefit.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Double precision benchmarks pitting Julia's builtin dot product, and code compiled with a variety of compilers: (Image: dot) What we just described is the core of the approach used by all these compilers. The variation in results is explained mostly by how they handle vectors with lengths that are not an integer multiple of W. I ran these on a computer with AVX512 so that W = 8. LLVM, the backend compiler of both Julia and Clang, shows rapid performance degredation as N % 4W increases, where N is the length of the vectors. This is because, to handle the remainder, it uses a scalar loop that runs as written: multiply and add single elements, one after the other. ","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Initially, GCC (gfortran) stumbled in throughput, because it does not use separate accumulation vectors by default except on Power, even with -funroll-loops. I compiled with the flags -fvariable-expansion-in-unroller --param max-variable-expansions-in-unroller=4 to allow for 4 accumulation vectors, yielding good performance.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"The Intel compilers have a secondary vectorized loop without any additional unrolling that masks off excess lanes beyond N (for when N isn't an integer multiple of W). LoopVectorization uses if/ifelse checks to determine how many extra vectors are needed, the last of which is masked.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Neither GCC nor LLVM use masks (without LoopVectorization's assitance).","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"I am not certain, but I believe Intel and GCC check for the vector's alignment, and align them if neccessary. Julia guarantees that the start of arrays beyond a certain size are aligned, so this is not an optimization I have implemented. But it may be worthwhile for handling large matrices with a number of rows that isn't an integer multiple of W. For such matrices, the first column may be aligned, but the next will not be.","category":"page"},{"location":"examples/dot_product/#Dot-Self","page":"Dot Products","title":"Dot-Self","text":"","category":"section"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"A related problem is taking the dot product of a vector with itself; taking the sum of squares is a common operation, for example when calculating the (log)density of independent normal variates:","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"function jselfdotavx(a)\n    s = zero(eltype(a))\n    @turbo for i ∈ eachindex(a)\n        s += a[i] * a[i]\n    end\n    s\nend","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Because we only need a single load per fma-instruction, we can now benefit from having 8 separate accumulators. For this reason, LoopVectorization now unrolls by 8 – it decides how much to unroll by comparing the bottlenecks on throughput with latency. The other compilers do not change their behavior, so now LoopVectorization has the advantage: (Image: selfdot) This algorithm may need refinement, because Julia (without LoopVectorization) only unrolls by 4, yet achieves roughly the same performance as LoopVectorization at multiples of 4W = 32, although performance declines rapidly from there due to the slow scalar loop. Performance for most is much higher – more GFLOPS – than the normal dot product, but still under half of the CPU's potential 131.2 GFLOPS, suggesting that some other bottlenecks are preventing the core from attaining 2 fmas per clock cycle. Note also that 8W = 64, so we don't really have enough iterations of the loop to amortize the overhead of performing the reductions of all these vectors into a single scalar. By the time the vectors are long enough to do this, we'll start running into memory bandwidth bottlenecks.","category":"page"},{"location":"devdocs/loopset_structure/#LoopSet-Structure","page":"LoopSet Structure","title":"LoopSet Structure","text":"","category":"section"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"The loopsets define loops as a set of operations that depend on one another, and also on loops. Cycles are not allowed, making it a directed acyclic graph. Let's use a set of nested loops performing matrix multiplication as an example. We can create a naive LoopSet from an expression (naive due to being created without access to any type information):","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> using LoopVectorization\n\njulia> AmulBq = :(for m ∈ 1:M, n ∈ 1:N\n           C[m,n] = zero(eltype(B))\n           for k ∈ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end);\n\njulia> lsAmulB = LoopVectorization.LoopSet(AmulBq);","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"This LoopSet consists of seven operations that define the relationships within the loop:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)\n7-element Array{LoopVectorization.Operation,1}:\n var\"##RHS#256\" = var\"##zero#257\"\n C[m, n] = var\"##RHS#256\"\n var\"##tempload#258\" = A[m, k]\n var\"##tempload#259\" = B[k, n]\n var\"##reduction#260\" = var\"##reductzero#261\"\n var\"##reduction#260\" = LoopVectorization.vfmadd_fast(var\"##tempload#258\", var\"##tempload#259\", var\"##reduction#260\")\n var\"##RHS#256\" = LoopVectorization.reduce_to_add(var\"##reduction#260\", var\"##RHS#256\")","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"The act of performing a \"reduction\" across a loop introduces a few extra operations that manage creating a \"zero\" with respect to the reduction, and then combining with the specified value using reduce_to_add, which performs any necessary type conversions, such as from an Vec vector-type to a scalar, if necessary. This simplifies code generation, by making the functions agnostic with respect to the actual vectorization decisions the library makes.","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"Each operation is listed as depending on a set of loop iteration symbols:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.loopdependencies.(LoopVectorization.operations(lsAmulB))\n7-element Array{Array{Symbol,1},1}:\n [:m, :n]\n [:m, :n]\n [:m, :k]\n [:k, :n]\n [:m, :n]\n [:m, :k, :n]\n [:m, :n]","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"We can also see which of the operations each of these operations depend on:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)[6]\nvar\"##reduction#260\" = LoopVectorization.vfmadd_fast(var\"##tempload#258\", var\"##tempload#259\", var\"##reduction#260\")\n\njulia> LoopVectorization.parents(ans)\n3-element Array{LoopVectorization.Operation,1}:\n var\"##tempload#258\" = A[m, k]\n var\"##tempload#259\" = B[k, n]\n var\"##reduction#260\" = var\"##reductzero#261\"","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"References to arrays are represented with an ArrayReferenceMeta data structure:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)[3].ref\nLoopVectorization.ArrayReferenceMeta(LoopVectorization.ArrayReference(:A, [:m, :k], Int8[0, 0]), Bool[1, 1], Symbol(\"##vptr##_A\"))","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"It contains the name of the parent array (:A), the indicies [:m,:k], and a boolean vector (Bool[1, 1]) indicating whether these indices are loop iterables. Note that the optimizer assumes arrays are column-major, and thus that it is efficient to read contiguous elements from the first index. In lower level terms, it means that high-throughput vmov instructions can be used rather than low-throughput gathers. Similar story for storing elements. When no axis has unit stride, the first given index will be the dummy Symbol(\"##DISCONTIGUOUSSUBARRAY##\").","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"warning: Warning\nCurrently, only single return values are supported (tuple destructuring is not supported in assignments).","category":"page"},{"location":"examples/filtering/#Image-Filtering","page":"Image Filtering","title":"Image Filtering","text":"","category":"section"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"Here, we convolve a small matrix kern with a larger matrix A, storing the results in out, using Julia's generic Cartesian Indexing:","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"using LoopVectorization, OffsetArrays, Images\nkern = Images.Kernel.gaussian((1, 1), (3, 3))\nA = rand(130,130);\nout = OffsetArray(similar(A, size(A) .- size(kernel) .+ 1), -1 .- kernel.offsets);\nfunction filter2davx!(out::AbstractMatrix, A::AbstractMatrix, kern)\n    @turbo for J in CartesianIndices(out)\n        tmp = zero(eltype(out))\n        for I ∈ CartesianIndices(kern)\n            tmp += A[I + J] * kern[I]\n        end\n        out[J] = tmp\n    end\n    out\nend","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"These are effectively four nested loops. For all the benchmarks, kern was 3 by 3, making it too small for vectorizing these loops to be particularly profitable. By vectorizing an outer loop instead, it can benefit from SIMD and also avoid having to do a reduction (horizontal addition) of a vector before storing in out, as the vectors can then be stored directly. (Image: dynamicfilter)","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"LoopVectorization achieved much better performance than all the alternatives, which tried vectorizing the inner loops. By making the compilers aware that the inner loops are too short to be worth vectorizing, we can get them to vectorize an outer loop instead. By defining the size of kern as constant in C and Fortran, and using size parameters in Julia, we can inform the compilers: (Image: staticsizefilter) Now all are doing much better than they were before, although still well shy of the 131.2 GFLOPS theoretical limit for the host CPU cores. While they all improved, two are lagging behind the main group:","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"ifort lags behind all the others except base Julia. I'll need to do more investigating to find out why.\nBase Julia. While providing static size information was enough for it to realize vectorizing the inner loops was not worth it, base Julia was seemingly the only one that didn't decide to vectorize an outer loop instead.","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"Manually unrolling the inner loops allows base Julia to vectorize, while the performance of all non-Julia variants was unchanged: (Image: unrolledfilter) LoopVectorization is currently limited to only unrolling two loops (but a third may be vectorized, effectively unrolling it by the length of the vectors). Manually unrolling two of the loops lets up to four loops be unrolled.","category":"page"},{"location":"#LoopVectorization.jl","page":"Home","title":"LoopVectorization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This documentation is for  LoopVectorization.jl. Please file an issue  if you run into any problems.","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"getting_started.md\",\n    \"examples/multithreading.md\",\n    \"examples/matrix_multiplication.md\",\n    \"examples/array_interface.md\",\n    \"examples/matrix_vector_ops.md\",\n    \"examples/dot_product.md\",\n    \"examples/datetime_arrays.md\",\n    \"examples/filtering.md\",\n    \"examples/special_functions.md\",\n    \"examples/sum_of_squared_error.md\",\n    \"vectorized_convenience_functions.md\",\n    \"performance_accuracy.md\",\n    \"future_work.md\",\n    \"devdocs/overview.md\",\n    \"devdocs/loopset_structure.md\",\n    \"devdocs/constructing_loopsets.md\",\n    \"devdocs/evaluating_loops.md\",\n    \"devdocs/lowering.md\"\n]\nDepth = 1","category":"page"}]
}
