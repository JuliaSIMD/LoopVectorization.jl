var documenterSearchIndex = {"docs":
[{"location":"examples/array_interface/#Array-Interface","page":"Array Interface","title":"Array Interface","text":"","category":"section"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"LoopVectorization uses ArrayInterface.jl to describe the memory layout of arrays. By supporting the interface, LoopVectorization will be able to support compatible AbstractArray types. StaticArrays.jl and HybridArrays.jl are two example libraries providing array types supporting the interface.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"StaticArrays.SArray itself is not compatible, because LoopVectorization needs access to a pointer. However,StaticArrays.MArrays are compatible. Loops featuring StaticArrays.StaticArray will result in a fall-back loop being executed that wasn't optimized by LoopVectorization, but instead simply had @inbounds @fastmath applied to the loop. This can often still yield reasonable to good performance, saving you from having to write more than one version of the loop to get good performance and correct behavior just because the array types happen to be different.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"By supporting the interface, using LoopVectorization can simplify implementing many operations like matrix multiply while still getting good performance. For example, instead of a few hundred lines of code to define matix multiplication in StaticArrays, one could simply write:","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"using StaticArrays, LoopVectorization\n\n@inline function AmulB!(C, A, B)\n    @turbo for n âˆˆ axes(C,2), m âˆˆ axes(C,1)\n        Cmn = zero(eltype(C))\n        for k âˆˆ axes(B,1)\n            Cmn += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\n    C\nend\n@inline AmulB(A::MMatrix{M,K,T}, B::MMatrix{K,N,T}) where {M,K,N,T} = AmulB!(MMatrix{M,N,T}(undef), A, B)\n@inline AmulB(A::SMatrix, B::SMatrix) = SMatrix(AmulB(MMatrix(A), MMatrix(B)))","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"Through converting back and fourth between SMatrix and MMatrix, we can still use LoopVectorization to implement SMatrix multiplication, and in most cases get better performance than the unrolled methods from the library. Unfortunately, it is still suboptimal because the compiler isn't able to elide the copying, but the temporaries are all stack-allocated, making the code allocateion free. We can benchmark our simple implementation vs the StaticArrays.SMatrix and StaticArrays.MMatrix methods:","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"using BenchmarkTools, LinearAlgebra, DataFrames, VegaLite\nBLAS.set_num_threads(1);\n\nmatdims(x::Integer) = (x, x, x)\nmatdims(x::NTuple{3}) = x\nmatflop(x::Integer) = 2x^3\nmatflop(x::NTuple{3}) = 2prod(x)\n\nfunction runbenches(sr, ::Type{T}, fa = identity, fb = identity) where {T}\n    bench_results = Matrix{Float64}(undef, length(sr), 4);\n    for (i,s) âˆˆ enumerate(sr)\n        M, K, N = matdims(s)\n        Am = @MMatrix rand(T, M, K)\n        Bm = @MMatrix rand(T, K, N)\n        As = Ref(SMatrix(Am));\n        Bs = Ref(SMatrix(Bm));\n        Css = fa(As[]) * fb(Bs[]);\n        Csl = AmulB(fa(As[]), fb(Bs[]))\n        Cms = similar(Css); mul!(Cms, fa(Am), fb(Bm));\n        Cml = similar(Css); AmulB!(Cml, fa(Am), fb(Bm));\n        @assert Array(Css) â‰ˆ Array(Csl) â‰ˆ Array(Cms) â‰ˆ Array(Cml) # Once upon a time Julia crashed on â‰ˆ for large static arrays\n        bench_results[i,1] = @belapsed $fa($As[]) * $fb($Bs[])\n        bench_results[i,2] = @belapsed AmulB($fa($As[]), $fb($Bs[]))\n        bench_results[i,3] = @belapsed mul!($Cms, $fa($Am), $fb($Bm))\n        bench_results[i,4] = @belapsed AmulB!($Cml, $fa($Am), $fb($Bm))\n        @show s, bench_results[i,:]\n    end\n    gflops = @. 1e-9 * matflop(sr) / bench_results\n    array_type = append!(fill(\"Static\", 2length(sr)), fill(\"Mutable\", 2length(sr)))\n    sa = fill(\"StaticArrays\", length(sr)); lv = fill(\"LoopVectorization\", length(sr));\n    matmul_lib = vcat(sa, lv, sa, lv);\n    sizes = reduce(vcat, (sr for _ âˆˆ 1:4))\n    DataFrame(\n        Size = sizes, Time = vec(bench_results), GFLOPS = vec(gflops),\n        ArrayType = array_type, MatmulLib = matmul_lib, MulType = array_type .* ' ' .* matmul_lib\n    )\nend\n\ndf = runbenches(1:24, Float64);\ndf |> @vlplot(:line, x = :Size, y = :GFLOPS, color = :MulType, height=640,width=960) |> save(\"sarraymatmul.svg\")","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"This yields: (Image: sarray_benchmarks) Our AmulB! for MMatrixes was the fastest at all sizes except 2x2, where it lost out to AmulB for SMatrix, which in turn was faster than the hundreds of lines of StaticArrays code at all sizes except 3x3,  5x5, and  6x6.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"Additionally, HybridArrays.jl can be used when we have a mix of dynamic and statically sized arrays. Maybe we want to multiply two matrices, where each element is a 3x3 matrix:","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"using HybridArrays, StaticArrays, LoopVectorization, BenchmarkTools\n\nA_static = [@SMatrix(rand(3,3)) for i in 1:32, j in 1:32];\nB_static = [@SMatrix(rand(3,3)) for i in 1:32, j in 1:32];\nC_static = similar(A_static);\n\nA_hybrid = HybridArray{Tuple{StaticArrays.Dynamic(),StaticArrays.Dynamic(),3,3}}(permutedims(reshape(reinterpret(Float64, A_static), (3,3,size(A_static)...)), (3,4,1,2)));\nB_hybrid = HybridArray{Tuple{StaticArrays.Dynamic(),StaticArrays.Dynamic(),3,3}}(permutedims(reshape(reinterpret(Float64, B_static), (3,3,size(B_static)...)), (3,4,1,2)));\nC_hybrid = HybridArray{Tuple{StaticArrays.Dynamic(),StaticArrays.Dynamic(),3,3}}(permutedims(reshape(reinterpret(Float64, C_static), (3,3,size(C_static)...)), (3,4,1,2)));\n\n# C is M x N x I x J\n# A is M x K x I x L\n# B is K x N x L x J\nfunction bmul!(C, A, B)\n    @turbo for n in axes(C,2), m in axes(C,1), j in axes(C,4), i in axes(C,3)\n        Cmnji = zero(eltype(C))\n        for k in axes(B,1), l in axes(B,3)\n            Cmnji += A[m,k,i,l] * B[k,n,l,j]\n        end\n        C[m,n,i,j] = Cmnji\n    end\nend","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"This yields","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"julia> @benchmark bmul!($C_hybrid, $A_hybrid, $B_hybrid)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     15.550 Î¼s (0.00% GC)\n  median time:      15.663 Î¼s (0.00% GC)\n  mean time:        15.685 Î¼s (0.00% GC)\n  maximum time:     50.286 Î¼s (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n  \njulia> @benchmark mul!($C_static, $A_static, $B_static)\nBenchmarkTools.Trial:\n  memory estimate:  336 bytes\n  allocs estimate:  6\n  --------------\n  minimum time:     277.736 Î¼s (0.00% GC)\n  median time:      278.035 Î¼s (0.00% GC)\n  mean time:        278.310 Î¼s (0.00% GC)\n  maximum time:     299.259 Î¼s (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> all(I -> C_hybrid[Tuple(I)[1],Tuple(I)[2],:,:] â‰ˆ C_static[I], CartesianIndices(C_static))\ntrue\n\njulia> length(C_hybrid) * size(B_hybrid,1) * size(B_hybrid,3) * 2e-9 / 15.55e-6 # GFLOPS loops + hybrid arrays\n113.79241157556271\n\njulia> length(C_hybrid) * size(B_hybrid,1) * size(B_hybrid,3) * 2e-9 / 277.736e-6 # GFLOPS LinearAlgebra.mul! + StaticArrays\n6.371057407034018","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"When using LoopVectorization + HybridArrays, you may often find that you often get the best performance when the leading dimensions are either an even multiple of 8, or relatively large. This will often mean not leading with a small static dimension, which is commonly best practice when not using LoopVectorization.","category":"page"},{"location":"examples/array_interface/","page":"Array Interface","title":"Array Interface","text":"If you happen to like tensor operations such as from this last example, you're also strongly encouraged to check out Tullio.jl which provides index-notation that is both much more convenient and much less error-prone than writing out loops, and uses both LoopVectorization (if you using LoopVectorization before @tullio) as well as multiple threads to maximize performance.","category":"page"},{"location":"devdocs/constructing_loopsets/#Constructing-LoopSets","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/#Loop-expressions","page":"Constructing LoopSets","title":"Loop expressions","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When applying @turbo to a loop expression, it creates a LoopSet without awareness to type information, and then condenses the information into a summary which is passed as type information to a generated function.","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"julia> @macroexpand @turbo for m âˆˆ 1:M, n âˆˆ 1:N\n           C[m,n] = zero(eltype(B))\n           for k âˆˆ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end\nquote\n    var\"##vptr##_C\" = LoopVectorization.stridedpointer(C)\n    var\"##vptr##_A\" = LoopVectorization.stridedpointer(A)\n    var\"##vptr##_B\" = LoopVectorization.stridedpointer(B)\n    begin\n        $(Expr(:gc_preserve, :(LoopVectorization._avx_!(Val{(0, 0)}(), Tuple{:numericconstant, Symbol(\"##zero#270\"), LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.constant, 0x00, 0x01), :LoopVectorization, :setindex!, LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000000, 0x0000000000000007, LoopVectorization.memstore, 0x01, 0x02), :LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000013, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x02, 0x03), :LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000032, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x03, 0x04), :numericconstant, Symbol(\"##reductzero#274\"), LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000003, 0x0000000000000000, LoopVectorization.constant, 0x00, 0x05), :LoopVectorization, :vfmadd_fast, LoopVectorization.OperationStruct(0x0000000000000132, 0x0000000000000003, 0x0000000000000000, 0x0000000000030405, LoopVectorization.compute, 0x00, 0x05), :LoopVectorization, :reduce_to_add, LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000003, 0x0000000000000000, 0x0000000000000601, LoopVectorization.compute, 0x00, 0x01)}, Tuple{LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffe03b), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000103, 0xffffffffffffffd6), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000302, 0xffffffffffffe056), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffffd6)}, Tuple{0, Tuple{}, Tuple{}, Tuple{}, Tuple{}, Tuple{(1, LoopVectorization.IntOrFloat), (5, LoopVectorization.IntOrFloat)}, Tuple{}}, (LoopVectorization.StaticLowerUnitRange{0}(M), LoopVectorization.StaticLowerUnitRange{0}(N), LoopVectorization.StaticLowerUnitRange{0}(K)), var\"##vptr##_C\", var\"##vptr##_A\", var\"##vptr##_B\", var\"##vptr##_C\")), :C, :A, :B))\n    end\nend","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When the corresponding method gets compiled for specific type of A, B, and C, the call to the @generated function _avx_! get compiled. This causes the summary to be reconstructed using the available type information. This type information can be used, for example, to realize an array has been transposed, and thus correctly identify which axis contains contiguous elements that are efficient to load from. This kind of information cannot be extracted from the raw expression, which is why these decisions are made when the method gets compiled for specific types via the @generated function _avx_!.","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"The three chief components of the summaries are the definitions of operations, e.g.:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":":LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000013, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x02, 0x03)","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"the referenced array objects:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffe03b)","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"and the set of loop bounds:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"(LoopVectorization.StaticLowerUnitRange{0}(M), LoopVectorization.StaticLowerUnitRange{0}(N), LoopVectorization.StaticLowerUnitRange{0}(K))","category":"page"},{"location":"devdocs/constructing_loopsets/#Broadcasting","page":"Constructing LoopSets","title":"Broadcasting","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When applying the @turbo macro to a broadcast expression, there are no explicit loops, and even the dimensionality of the operation is unknown.  Consequently the LoopSet object must be constructed at compile time. The function and involved operations are their relationships are straightforward to infer from the structure of nested broadcasts:","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"julia> Meta.@lower @. f(g(a,b) + c) / d\n:($(Expr(:thunk, CodeInfo(\n    @ none within `top-level scope'\n1 â”€ %1 = Base.broadcasted(g, a, b)\nâ”‚   %2 = Base.broadcasted(+, %1, c)\nâ”‚   %3 = Base.broadcasted(f, %2)\nâ”‚   %4 = Base.broadcasted(/, %3, d)\nâ”‚   %5 = Base.materialize(%4)\nâ””â”€â”€      return %5\n))))\n\njulia> @macroexpand @turbo @. f(g(a,b) + c) / d\nquote\n    var\"##262\" = Base.broadcasted(g, a, b)\n    var\"##263\" = Base.broadcasted(+, var\"##262\", c)\n    var\"##264\" = Base.broadcasted(f, var\"##263\")\n    var\"##265\" = Base.broadcasted(/, var\"##264\", d)\n    var\"##266\" = LoopVectorization.vmaterialize(var\"##265\", Val{:Main}())\nend","category":"page"},{"location":"devdocs/constructing_loopsets/","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"These nested broadcasted objects already express information very similar to what the LoopSet objects hold. The dimensionality of the objects provides the information on the associated loop dependencies, but again this information is available only when the method is compiled for specific types. The @generated function vmaterialize constructs the LoopSet by recursively evaluating add_broadcast! on all the fields.","category":"page"},{"location":"examples/matrix_vector_ops/#Matrix-Vector-Operations","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"","category":"section"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Here I'll discuss a variety of Matrix-vector operations, naturally starting with matrix-vector multiplication.","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"function jgemvavx!(ğ², ğ€, ğ±)\n    @turbo for i âˆˆ eachindex(ğ²)\n        ğ²i = zero(eltype(ğ²))\n        for j âˆˆ eachindex(ğ±)\n            ğ²i += ğ€[i,j] * ğ±[j]\n        end\n        ğ²[i] = ğ²i\n    end\nend","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Using a square Size x Size matrix ğ€, we find the following results. (Image: Amulvb)","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"If ğ€ is transposed, or equivalently, if we're instead computing x * ğ€: (Image: Atmulvb)","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Finally, the three-argument dot product y' * ğ€ * x: (Image: dot3)","category":"page"},{"location":"examples/matrix_vector_ops/","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"The performance impact of alignment is dramatic here.","category":"page"},{"location":"devdocs/lowering/#Lowering","page":"Lowering","title":"Lowering","text":"","category":"section"},{"location":"devdocs/lowering/","page":"Lowering","title":"Lowering","text":"The first step to lowering is picking a strategy for lowering the loops. Then a Julia expression is created following that strategy, converting each of the operations into Julia expressions. This task is made simpler via multiple dispatch making the lowering of the components independent of the larger picture. For example, a load will look like","category":"page"},{"location":"devdocs/lowering/","page":"Lowering","title":"Lowering","text":"vload(vptr_A, (i,j,k))","category":"page"},{"location":"devdocs/lowering/","page":"Lowering","title":"Lowering","text":"with the behavior of this load determined by the types of the arguments. Vectorization is expressed by making an index a _MM{W} type, rather than an integer, and operations with it will either produce another _MM{W} when it will still correspond to contiguous loads, or an Vec{W,<:Integer} if the resulting loads will be discontiguous, so that a gather or scatter! will be used. If all indexes are simply integers, then this produces a scalar load or store.","category":"page"},{"location":"examples/special_functions/#Special-Functions","page":"Special Functions","title":"Special Functions","text":"","category":"section"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"LoopVectorization supports vectorizing many special functions, for example, to calculate the log determinant of a triangular matrix:","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"function logdettriangle(B::Union{LowerTriangular,UpperTriangular})\n    A = parent(B) # using a triangular matrix would fall back to the default loop.\n    ld = zero(eltype(A))\n    @turbo for n âˆˆ axes(A,1)\n        ld += log(A[n,n])\n    end\n    ld\nend","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"(Image: selfdot)","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"While Intel's proprietary compilers do the best, LoopVectorization performs very well among open source alternatives. A complicating factor to the above benchmark is that in accessing the diagonals, we are not accessing contiguous elements. A benchmark simply exponentiating a vector shows that gcc also has efficient special function vectorization, but that the autovectorizer disagrees with the discontiguous memory acesses:","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"(Image: selfdot)","category":"page"},{"location":"examples/special_functions/","page":"Special Functions","title":"Special Functions","text":"The similar performance between gfortran and LoopVectorization at multiples of 8 is no fluke: on Linux systems with a recent GLIBC, SLEEFPirates.jl â€“ which LoopVectorization depends on to vectorize these special functions â€“ looks for the GNU vector library and uses these functions if available. Otherwise, it will use native Julia implementations that tend to be slower. As the modulus of vector length and vector width (8, on the host system thanks to AVX512) increases, gfortran shows the performance degredation pattern typical of LLVM-vectorized code.","category":"page"},{"location":"devdocs/evaluating_loops/#Determining-the-strategy-for-evaluating-loops","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"","category":"section"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The heart of the optimizatizations performed by LoopVectorization are given in the determinestrategy.jl file utilizing instruction costs specified in costs.jl. Essentially, it estimates the cost of different means of evaluating the loops. It iterates through the different possible loop orders, as well as considering which loops to unroll, and which to vectorize. It will consider unrolling 1 or 2 loops (but it could settle on unrolling by a factor of 1, i.e. not unrolling), and vectorizing 1.","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The cost estimate is based on the costs of individual instructions and the number of times each one needs to be executed for the given strategy. The instruction cost can be broken into several components:","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The scalar latency is the minimum delay, in clock cycles, associated with the instruction. Think of it as the delay from turning on the water to when water starts coming out the hose.\nThe reciprocal throughput is similar to the latency, but it measures the number of cycles per operation when many of the same operation are repeated in sequence.  Continuing our hose analogy, think of it as the inverse of the flow rate at steady-state. It is typically â‰¤ the scalar latency.\nThe register pressure measures the register consumption by the operation","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Data on individual instructions for specific architectures can be found on Agner Fog's website. Most of the costs used were those for the Skylake-X architecture.","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Examples of how these come into play:","category":"page"},{"location":"devdocs/evaluating_loops/","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Vectorizing a loop will result in each instruction evaluating multiple iterations, but the costs of loads and stores will change based on the memory layouts of the accessed arrays.\nUnrolling can help reduce the number of times an operation must be performed, for example if it can allow us to reuse memory multiple times rather than reloading it every time it is needed.\nWhen there is a reduction, such as performing a sum, there is a dependency chain. Each + has to wait for the previous + to finish executing before it can begin, thus execution time is bounded by latency rather than minimum of the throughput of the + and load operations. By unrolling the loop, we can create multiple independent dependency chains.","category":"page"},{"location":"future_work/#Future-Plans","page":"Future Work","title":"Future Plans","text":"","category":"section"},{"location":"future_work/","page":"Future Work","title":"Future Work","text":"Future plans for LoopVectorization:","category":"page"},{"location":"future_work/","page":"Future Work","title":"Future Work","text":"Support triangular iteration spaces.\nIdentify obvious loop-carried dependencies like A[j] and A[j-1].\nBe able to generate optimized kernels from simple loop-based implementations of operations like Cholesky decompositions or solving triangular systems of equations.\nModel memory and CPU-cache to possibly insert extra loops and packing of data when deemed profitable.\nTrack types of individual operations in the loops. Currently, multiple types in loops aren't really handled, so this is a bit brittle at the moment.\nHandle loops where arrays contain non-primitive types (e.g., Complex numbers) well.","category":"page"},{"location":"future_work/","page":"Future Work","title":"Future Work","text":"Contributions are more than welcome, and I would be happy to assist if anyone would like to take a stab at any of these. Otherwise, while LoopVectorization is a core component to much of my work, so that I will continue developing it, I have many other projects that require active development, so it will be a long time before I am able to address these myself.","category":"page"},{"location":"devdocs/overview/#Developer-Overview","page":"Developer Overview","title":"Developer Overview","text":"","category":"section"},{"location":"devdocs/overview/","page":"Developer Overview","title":"Developer Overview","text":"Here I will try to explain how the library works for the curious or any would-be contributors.","category":"page"},{"location":"devdocs/overview/","page":"Developer Overview","title":"Developer Overview","text":"The library uses a LoopSet object to model loops. The key components of the library can be divided into:","category":"page"},{"location":"devdocs/overview/","page":"Developer Overview","title":"Developer Overview","text":"Defining the LoopSet objects.\nConstructing the LoopSet objects.\nDetermining the strategy of how to evaluate loops.\nLowering the loopset object into a Julia Expr following a strategy.","category":"page"},{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Macros","page":"API reference","title":"Macros","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"@turbo\n@tturbo","category":"page"},{"location":"api/#LoopVectorization.@turbo","page":"API reference","title":"LoopVectorization.@turbo","text":"@turbo\n\nAnnotate a for loop, or a set of nested for loops whose bounds are constant across iterations, to optimize the computation. For example:\n\nfunction AmulB!(C, A, B)\n    @turbo for m âˆˆ 1:size(A,1), n âˆˆ 1:size(B,2)\n        Câ‚˜â‚™ = zero(eltype(C))\n        for k âˆˆ 1:size(A,2)\n            Câ‚˜â‚™ += A[m,k] * B[k,n]\n        end\n        C[m,n] = Câ‚˜â‚™\n    end\nend\n\nThe macro models the set of nested loops, and chooses an ordering of the three loops to minimize predicted computation time.\n\nIt may also apply to broadcasts:\n\njulia> using LoopVectorization\n\njulia> a = rand(100);\n\njulia> b = @turbo exp.(2 .* a);\n\njulia> c = similar(b);\n\njulia> @turbo @. c = exp(2a);\n\njulia> b â‰ˆ c\ntrue\n\nExtended help\n\nAdvanced users can customize the implementation of the @turbo-annotated block using keyword arguments:\n\n@turbo inline=false unroll=2 body\n\nwhere body is the code of the block (e.g., for ... end).\n\nthread is either a Boolean, or an integer. The integer's value indicates the number of threads to use. It is clamped to be between 1 and min(Threads.nthreads(),LoopVectorization.num_cores()). false is equivalent to 1, and true is equivalent to min(Threads.nthreads(),LoopVectorization.num_cores()).\n\ninline is a Boolean. When true, body will be directly inlined into the function (via a forced-inlining call to _avx_!). When false, it wont force inlining of the call to _avx_! instead, letting Julia's own inlining engine determine whether the call to _avx_! should be inlined. (Typically, it won't.) Sometimes not inlining can lead to substantially worse code generation, and >40% regressions, even in very large problems (2-d convolutions are a case where this has been observed). One can find some circumstances where inline=true is faster, and other circumstances where inline=false is faster, so the best setting may require experimentation. By default, the macro tries to guess. Currently the algorithm is simple: roughly, if there are more than two dynamically sized loops or and no convolutions, it will probably not force inlining. Otherwise, it probably will.\n\ncheck_empty (default is false) determines whether or not it will check if any of the iterators are empty. If false, you must ensure yourself that they are not empty, else the behavior of the loop is undefined and (like with @inbounds) segmentation faults are likely.\n\nunroll is an integer that specifies the loop unrolling factor, or a tuple (uâ‚, uâ‚‚) = (4, 2) signaling that the generated code should unroll more than one loop. uâ‚ is the unrolling factor for the first unrolled loop and uâ‚‚ for the next (if present), but it applies to the loop ordering and unrolling that will be chosen by LoopVectorization, not the order in body. uáµ¢=0 (the default) indicates that LoopVectorization should pick its own value, and uáµ¢=-1 disables unrolling for the correspond loop.\n\nThe @turbo macro also checks the array arguments using LoopVectorization.check_args to try and determine if they are compatible with the macro. If check_args returns false, a fall back loop annotated with @inbounds and @fastmath is generated. Note that VectorizationBase provides functions such as vadd and vmul that will ignore @fastmath, preserving IEEE semantics both within @turbo and @fastmath. check_args currently returns false for some wrapper types like LinearAlgebra.UpperTriangular, requiring you to use their parent. Triangular loops aren't yet supported.\n\n\n\n\n\n","category":"macro"},{"location":"api/#LoopVectorization.@tturbo","page":"API reference","title":"LoopVectorization.@tturbo","text":"Equivalent to @turbo, except it adds thread=true as the first keyword argument. Note that later arguments take precendence.\n\nMeant for convenience, as @tturbo is shorter than @turbo thread=true.\n\n\n\n\n\n","category":"macro"},{"location":"api/#map-like-constructs","page":"API reference","title":"map-like constructs","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"vmap\nvmap!\nvmapnt\nvmapnt!\nvmapntt\nvmapntt!","category":"page"},{"location":"api/#LoopVectorization.vmap","page":"API reference","title":"LoopVectorization.vmap","text":"vmap(f, a::AbstractArray)\nvmap(f, a::AbstractArray, b::AbstractArray, ...)\n\nSIMD-vectorized map, applying f to each element of a (or paired elements of a, b, ...) and returning a new array.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmap!","page":"API reference","title":"LoopVectorization.vmap!","text":"vmap!(f, destination, a::AbstractArray)\nvmap!(f, destination, a::AbstractArray, b::AbstractArray, ...)\n\nVectorized-map!, applying f to each element of a (or paired elements of a, b, ...) and storing the result in destination.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapnt","page":"API reference","title":"LoopVectorization.vmapnt","text":"vmapnt(f, a::AbstractArray)\nvmapnt(f, a::AbstractArray, b::AbstractArray, ...)\n\nA \"non-temporal\" variant of vmap. This can improve performance in cases where destination will not be needed soon.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapnt!","page":"API reference","title":"LoopVectorization.vmapnt!","text":"vmapnt!(::Function, dest, args...)\n\nThis is a vectorized map implementation using nontemporal store operations. This means that the write operations to the destination will not go to the CPU's cache. If you will not immediately be reading from these values, this can improve performance because the writes won't pollute your cache. This can especially be the case if your arguments are very long.\n\njulia> using LoopVectorization, BenchmarkTools\njulia> x = rand(10^8); y = rand(10^8); z = similar(x);\njulia> f(x,y) = exp(-0.5abs2(x - y))\nf (generic function with 1 method)\njulia> @benchmark map!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     439.613 ms (0.00% GC)\n  median time:      440.729 ms (0.00% GC)\n  mean time:        440.695 ms (0.00% GC)\n  maximum time:     441.665 ms (0.00% GC)\n  --------------\n  samples:          12\n  evals/sample:     1\njulia> @benchmark vmap!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     178.147 ms (0.00% GC)\n  median time:      178.381 ms (0.00% GC)\n  mean time:        178.430 ms (0.00% GC)\n  maximum time:     179.054 ms (0.00% GC)\n  --------------\n  samples:          29\n  evals/sample:     1\njulia> @benchmark vmapnt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     144.183 ms (0.00% GC)\n  median time:      144.338 ms (0.00% GC)\n  mean time:        144.349 ms (0.00% GC)\n  maximum time:     144.641 ms (0.00% GC)\n  --------------\n  samples:          35\n  evals/sample:     1\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapntt","page":"API reference","title":"LoopVectorization.vmapntt","text":"vmapntt(f, a::AbstractArray)\nvmapntt(f, a::AbstractArray, b::AbstractArray, ...)\n\nA threaded variant of vmapnt.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapntt!","page":"API reference","title":"LoopVectorization.vmapntt!","text":"vmapntt!(::Function, dest, args...)\n\nA threaded variant of vmapnt!.\n\n\n\n\n\n","category":"function"},{"location":"api/#filter-like-constructs","page":"API reference","title":"filter-like constructs","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"vfilter\nLoopVectorization.vfilter!","category":"page"},{"location":"api/#LoopVectorization.vfilter","page":"API reference","title":"LoopVectorization.vfilter","text":"vfilter(f, a::AbstractArray)\n\nSIMD-vectorized filter, returning an array containing the elements of a for which f return true.\n\nThis function requires AVX512 to be faster than Base.filter, as it adds compressstore instructions.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vfilter!","page":"API reference","title":"LoopVectorization.vfilter!","text":"vfilter!(f, a::AbstractArray)\n\nSIMD-vectorized filter!, removing the element of a for which f is false.\n\n\n\n\n\n","category":"function"},{"location":"api/#reduce-like-constructs","page":"API reference","title":"reduce-like constructs","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"vreduce\nvmapreduce","category":"page"},{"location":"api/#LoopVectorization.vreduce","page":"API reference","title":"LoopVectorization.vreduce","text":"vreduce(op, destination, A::DenseArray...)\n\nVectorized version of reduce. Reduces the array A using the operator op.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapreduce","page":"API reference","title":"LoopVectorization.vmapreduce","text":"vmapreduce(f, op, A::DenseArray...)\n\nVectorized version of mapreduce. Applies f to each element of the arrays A, and reduces the result with op.\n\n\n\n\n\n","category":"function"},{"location":"devdocs/reference/#Internals-reference","page":"Internals reference","title":"Internals reference","text":"","category":"section"},{"location":"devdocs/reference/#Operation-types","page":"Internals reference","title":"Operation types","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.OperationType\nLoopVectorization.constant\nLoopVectorization.memload\nLoopVectorization.compute\nLoopVectorization.memstore\nLoopVectorization.loopvalue","category":"page"},{"location":"devdocs/reference/#LoopVectorization.OperationType","page":"Internals reference","title":"LoopVectorization.OperationType","text":"OperationType is an @enum for classifying supported operations that can appear in @turbo blocks. Type LoopVectorization.OperationType to see the different types.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.constant","page":"Internals reference","title":"LoopVectorization.constant","text":"An operation setting a variable to a constant value (e.g., a = 0.0)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.memload","page":"Internals reference","title":"LoopVectorization.memload","text":"An operation setting a variable from a memory location (e.g., a = A[i,j])\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.compute","page":"Internals reference","title":"LoopVectorization.compute","text":"An operation computing a new value from one or more variables (e.g., a = b + c)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.memstore","page":"Internals reference","title":"LoopVectorization.memstore","text":"An operation storing a value to a memory location (e.g., A[i,j] = a)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.loopvalue","page":"Internals reference","title":"LoopVectorization.loopvalue","text":"loopvalue indicates an loop variable (i in for i in ...). These are the \"parents\" of compute operations that involve the loop variables.\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#Operation","page":"Internals reference","title":"Operation","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.Operation","category":"page"},{"location":"devdocs/reference/#LoopVectorization.Operation","page":"Internals reference","title":"LoopVectorization.Operation","text":"Operation\n\nA structure to encode a particular action occuring inside an @turbo block.\n\nFields\n\nidentifier::Int64\nA unique identifier for this operation. identifer(op::Operation) returns the index of this operation within operations(ls::LoopSet).\nvariable::Symbol\nThe name of the variable storing the result of this operation. For a = val this would be :a. For array assignments A[i,j] = val this would be :A.\nelementbytes::Int64\nIntended to be the size of the result, in bytes. Often inaccurate, not to be relied on.\ninstruction::LoopVectorization.Instruction\nThe specific operator, e.g., identity or +\nnode_type::LoopVectorization.OperationType\nThe OperationType associated with this operation\ndependencies::Vector{Symbol}\nThe loop variables this operation depends on\nreduced_deps::Vector{Symbol}\nAdditional loop dependencies that must execute before this operation can be performed successfully (often needed in reductions)\nparents::Vector{LoopVectorization.Operation}\nOperations whose result this operation depends on\nchildren::Vector{LoopVectorization.Operation}\nOperations who depend on this result\nref::LoopVectorization.ArrayReferenceMeta\nFor memload or memstore, encodes the array location\nmangledvariable::Symbol\ngensymmed name of result.\nreduced_children::Vector{Symbol}\nLoop variables that consumers of this operation depend on. Often used in reductions to replicate assignment of initializers when unrolling.\nuâ‚unrolled::Bool\nCached value for whether uâ‚loopsym âˆˆ loopdependencies(op)\nuâ‚‚unrolled::Bool\nCached value for whether uâ‚‚loopsym âˆˆ loopdependencies(op)\nvectorized::Bool\nCached value for whether vectorized âˆˆ loopdependencies(op)\nrejectcurly::Bool\nCached value for whether or not to lower memop using Unrolled\nrejectinterleave::Bool\nCached value for whether or not to lower memop by interleaving it with offset operations\n\nExample\n\njulia> using LoopVectorization\n\njulia> AmulBq = :(for m âˆˆ 1:M, n âˆˆ 1:N\n           C[m,n] = zero(eltype(B))\n           for k âˆˆ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end);\n\njulia> lsAmulB = LoopVectorization.LoopSet(AmulBq);\n\njulia> LoopVectorization.operations(lsAmulB)\n6-element Vector{LoopVectorization.Operation}:\n var\"##RHS#245\" = var\"##zero#246\"\n C[m, n] = var\"##RHS#245\"\n var\"##tempload#248\" = A[m, k]\n var\"##tempload#249\" = B[k, n]\n var\"##RHS#245\" = LoopVectorization.vfmadd(var\"##tempload#248\", var\"##tempload#249\", var\"##RHS#245\")\n var\"##RHS#245\" = LoopVectorization.identity(var\"##RHS#245\")\n\nEach one of these lines is a pretty-printed Operation.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Instructions-and-costs","page":"Internals reference","title":"Instructions and costs","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.Instruction\nLoopVectorization.InstructionCost","category":"page"},{"location":"devdocs/reference/#LoopVectorization.Instruction","page":"Internals reference","title":"LoopVectorization.Instruction","text":"Instruction\n\nInstruction represents a function via its module and symbol. It is similar to a GlobalRef and may someday be replaced by GlobalRef.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.InstructionCost","page":"Internals reference","title":"LoopVectorization.InstructionCost","text":"InstructionCost\n\nStore parameters related to performance for individual CPU instructions.\n\nscaling::Float64\nA flag indicating how instruction cost scales with vector width (128, 256, or 512 bits)\nscalar_reciprocal_throughput::Float64\nThe number of clock cycles per operation when many of the same operation are repeated in sequence. Think of it as the inverse of the flow rate at steady-state. It is typically â‰¤ the scalar_latency.\nscalar_latency::Int64\nThe minimum delay, in clock cycles, associated with the instruction. Think of it as the delay from turning on a faucet to when water starts coming out the end of the pipe. See also scalar_reciprocal_throughput.\nregister_pressure::Int64\nNumber of floating-point registered used\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Array-references","page":"Internals reference","title":"Array references","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.ArrayReference\nLoopVectorization.ArrayReferenceMeta","category":"page"},{"location":"devdocs/reference/#LoopVectorization.ArrayReference","page":"Internals reference","title":"LoopVectorization.ArrayReference","text":"ArrayReference\n\nA type for encoding an array reference A[i,j] occurring inside an @turbo block.\n\nFields\n\narray::Symbol\nThe array variable\nindices::Vector{Symbol}\nThe list of indices (e.g., [:i, :j]), or name(op) for computed indices.\noffsets::Vector{Int8}\nIndex offset, e.g., a[i+7] would store the 7. offsets is also used to help identify opportunities for avoiding reloads, for example in y[i] = x[i] - x[i-1], the previous load x[i-1] can be \"carried over\" to the next iteration. Only used for small (Int8) offsets.\nstrides::Vector{Int8}\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.ArrayReferenceMeta","page":"Internals reference","title":"LoopVectorization.ArrayReferenceMeta","text":"ArrayReferenceMeta\n\nA type similar to ArrayReference but holding additional information.\n\nFields\n\nref::LoopVectorization.ArrayReference\nThe ArrayReference\nloopedindex::Vector{Bool}\nA vector of Bools indicating whether each index is a loop variable (false for operation-computed indices)\nptr::Symbol\nVariable holding the pointer to the array's underlying storage\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Condensed-types","page":"Internals reference","title":"Condensed types","text":"","category":"section"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"These are used when encoding the @turbo block as a type parameter for passing through to the @generated function.","category":"page"},{"location":"devdocs/reference/","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.ArrayRefStruct\nLoopVectorization.OperationStruct","category":"page"},{"location":"devdocs/reference/#LoopVectorization.ArrayRefStruct","page":"Internals reference","title":"LoopVectorization.ArrayRefStruct","text":"ArrayRefStruct\n\nA condensed representation of an ArrayReference. It supports array-references with up to 8 indexes, where the data for each consecutive index is packed into corresponding 8-bit fields of index_types (storing the enum IndexType), indices (the id for each index symbol), and offsets (currently unused).\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.OperationStruct","page":"Internals reference","title":"LoopVectorization.OperationStruct","text":"OperationStruct\n\nA condensed representation of an Operation.\n\n\n\n\n\n","category":"type"},{"location":"vectorized_convenience_functions/#Convenient-Vectorized-Functions","page":"Vectorized Convenience Functions","title":"Convenient Vectorized Functions","text":"","category":"section"},{"location":"vectorized_convenience_functions/#vmap","page":"Vectorized Convenience Functions","title":"vmap","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"This is simply a vectorized map function.","category":"page"},{"location":"vectorized_convenience_functions/#vmapnt-and-vmapntt","page":"Vectorized Convenience Functions","title":"vmapnt and vmapntt","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"These are like vmap, but use non-temporal (streaming) stores into the destination, to avoid polluting the cache. Likely to yield a performance increase if you wont be reading the values soon.","category":"page"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> f(x,y) = exp(-0.5abs2(x - y))\nf (generic function with 1 method)\n\njulia> x = rand(10^8); y = rand(10^8); z = similar(x);\n\njulia> @benchmark map!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     442.614 ms (0.00% GC)\n  median time:      443.750 ms (0.00% GC)\n  mean time:        443.664 ms (0.00% GC)\n  maximum time:     444.730 ms (0.00% GC)\n  --------------\n  samples:          12\n  evals/sample:     1\n\njulia> @benchmark vmap!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     177.257 ms (0.00% GC)\n  median time:      177.380 ms (0.00% GC)\n  mean time:        177.423 ms (0.00% GC)\n  maximum time:     177.956 ms (0.00% GC)\n  --------------\n  samples:          29\n  evals/sample:     1\n\njulia> @benchmark vmapnt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     143.521 ms (0.00% GC)\n  median time:      143.639 ms (0.00% GC)\n  mean time:        143.645 ms (0.00% GC)\n  maximum time:     143.821 ms (0.00% GC)\n  --------------\n  samples:          35\n  evals/sample:     1\n\njulia> Threads.nthreads()\n36\n\njulia> @benchmark vmapntt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  25.69 KiB\n  allocs estimate:  183\n  --------------\n  minimum time:     30.065 ms (0.00% GC)\n  median time:      30.130 ms (0.00% GC)\n  mean time:        30.146 ms (0.00% GC)\n  maximum time:     31.277 ms (0.00% GC)\n  --------------\n  samples:          166\n  evals/sample:     1","category":"page"},{"location":"vectorized_convenience_functions/#vfilter","page":"Vectorized Convenience Functions","title":"vfilter","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"This function requires LLVM 7 or greater, and is only likly to give better performance if your CPU has AVX512. This is because it uses the compressed store intrinsic, which was added in LLVM 7. AVX512 provides a corresponding instruction, making the operation fast, while other instruction sets must emulate it, and thus are likely to get similar performance with LoopVectorization.vfilter as they do from Base.filter.","category":"page"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(997);\n\njulia> y1 = filter(a -> a > 0.7, x);\n\njulia> y2 = vfilter(a -> a > 0.7, x);\n\njulia> y1 == y2\ntrue\n\njulia> @benchmark filter(a -> a > 0.7, $x)\nBenchmarkTools.Trial:\n  memory estimate:  7.94 KiB\n  allocs estimate:  1\n  --------------\n  minimum time:     955.389 ns (0.00% GC)\n  median time:      1.050 Î¼s (0.00% GC)\n  mean time:        1.191 Î¼s (9.72% GC)\n  maximum time:     82.799 Î¼s (94.92% GC)\n  --------------\n  samples:          10000\n  evals/sample:     18\n\njulia> @benchmark vfilter(a -> a > 0.7, $x)\nBenchmarkTools.Trial:\n  memory estimate:  7.94 KiB\n  allocs estimate:  1\n  --------------\n  minimum time:     477.487 ns (0.00% GC)\n  median time:      575.166 ns (0.00% GC)\n  mean time:        711.526 ns (17.87% GC)\n  maximum time:     9.257 Î¼s (79.17% GC)\n  --------------\n  samples:          10000\n  evals/sample:     193","category":"page"},{"location":"vectorized_convenience_functions/#vmapreduce","page":"Vectorized Convenience Functions","title":"vmapreduce","text":"","category":"section"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"Vectorized version of mapreduce. vmapreduce(f, op, a, b, c) applies f(a[i], b[i], c[i]) for i in eachindex(a,b,c), reducing the results to a scalar with op.","category":"page"},{"location":"vectorized_convenience_functions/","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(127); y = rand(127);\n\njulia> @btime vmapreduce(hypot, +, $x, $y)\n  191.420 ns (0 allocations: 0 bytes)\n96.75538300513509\n\njulia> @btime mapreduce(hypot, +, $x, $y)\n  1.777 Î¼s (5 allocations: 1.25 KiB)\n96.75538300513509","category":"page"},{"location":"examples/multithreading/#Multithreading","page":"Multithreading","title":"Multithreading","text":"","category":"section"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization can multithread loops if you pass the argument @turbo thread=true for ... end or equivalently use @tturbo. By default, thread = false, which runs only a single thread. You can also supply a numerical argument to set an upper bound on the number of threads, e.g. @turbo thread=8 for ... end will use up to min(8,Threads.nthreads(),VectorizationBase.num_cores()) threads. VectorizationBase.num_cores() uses Hwloc.jl to get the number of physical cores. Currently, this only works for for loops, but support for broadcasting will come.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Lets look at a few benchmarks.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Taking the first example from the ThreadsX.jl README:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function relative_prime_count(x, N)\n    c = 0\n    @tturbo for i âˆˆ 1:N\n        c += gcd(x, i) == 1\n    end\n    c\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Benchmarking them:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> @btime ThreadsX.sum(gcd(42, i) == 1 for i in 1:10_000)\n  130.928 Î¼s (3097 allocations: 240.39 KiB)\n2857\n\njulia> @btime relative_prime_count(42, 10_000)\n  3.376 Î¼s (0 allocations: 0 bytes)\n2857","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Note that much of the performance difference here is thanks to SIMD, which requires AVX512 for good performance (trailing_zeros, required by gcd, needs AVX512 for a SIMD version). LoopVectorization is a good choice for loops (a) amenable to SIMD (b) where all arrays are dense and (c) a static schedule would work well. Generally, this means loops built up of relatively primitive arithmetic operations (e.g. +, /, or log), and not, for example, solving differential equations.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"I'll make comparisons with OpenMP through the rest of this, starting with a simple dot product to focus on threading overhead:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function dot_tturbo(a::AbstractArray{T}, b::AbstractArray{T}) where {T <: Real}\n    s = zero(T)\n    @tturbo for i âˆˆ eachindex(a,b)\n        s += a[i] * b[i]\n    end\n    s\nend\nfunction dotbaseline(a::AbstractArray{T}, b::AbstractArray{T}) where {T}\n    s = zero(T)\n    @fastmath @inbounds @simd for i âˆˆ eachindex(a,b)\n        s += a[i]' * b[i]\n    end\n    s\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"In C:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"#include<omp.h>\n//  gcc -Ofast -march=native -mprefer-vector-width=512 -fopenmp -shared -fPIC openmp.c -o libomptest.so\ndouble dot(double* a, double* b, long N){\n  double s = 0.0;\n  #pragma omp parallel for reduction(+: s)\n  for(long n = 0; n < N; n++){\n    s += a[n]*b[n];\n  }\n  return s;\n}","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Wrapping it in Julia is straightforward, after compiling:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"using Libdl; const OPENMPTEST = joinpath(pkgdir(LoopVectorization), \"benchmark\", \"libomptest.$(Libdl.dlext)\");\ncdot(a::AbstractVector{Float64},b::AbstractVector{Float64}) = @ccall OPENMPTEST.dot(a::Ref{Float64}, b::Ref{Float64}, length(a)::Clong)::Float64","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Trying out one size to give a perspective on scale:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> N = 10_000; x = rand(N); y = rand(N);\n\njulia> @btime dot($x, $y) # LinearAlgebra\n  1.114 Î¼s (0 allocations: 0 bytes)\n2480.296446711209\n\njulia> @btime dot_turbo($x, $y)\n  761.621 ns (0 allocations: 0 bytes)\n2480.296446711209\n\njulia> @btime dot_tturbo($x, $y)\n  622.723 ns (0 allocations: 0 bytes)\n2480.296446711209\n\njulia> @btime dot_baseline($x, $y)\n  1.294 Î¼s (0 allocations: 0 bytes)\n2480.2964467112097\n\njulia> @btime cdot($x, $y)\n  6.109 Î¼s (0 allocations: 0 bytes)\n2480.2964467112092","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"All these times are fairly fast; wait(Threads.@spawn 1+1) will typically take much longer than even @cdot did here. (Image: realdot)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Now let's look at a more complex example:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function dot_tturbo(ca::AbstractVector{Complex{T}}, cb::AbstractVector{Complex{T}}) where {T}\n    a = reinterpret(reshape, T, ca)\n    b = reinterpret(reshape, T, cb)\n    re = zero(T); im = zero(T)\n    @tturbo for i âˆˆ axes(a,2) # adjoint(a[i]) * b[i]\n        re += a[1,i] * b[1,i] + a[2,i] * b[2,i]\n        im += a[1,i] * b[2,i] - a[2,i] * b[1,i]\n    end\n    Complex(re, im)\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization currently only supports arrays of type T <: Union{Bool,Base.HWReal}. So to support Complex{T}, we reinterpret the arrays and then write out the corresponding operations. The plan is to eventually have LoopVectorization do this automatically, but for now we require this workaround. Corresponding C:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"void cdot(double* c, double* a, double* b, long N){\n  double r = 0.0, i = 0.0;\n  #pragma omp parallel for reduction(+: r, i)\n  for(long n = 0; n < N; n++){\n    r += a[2*n] * b[2*n  ] + a[2*n+1] * b[2*n+1];\n    i += a[2*n] * b[2*n+1] - a[2*n+1] * b[2*n  ];\n  }\n  c[0] = r;\n  c[1] = i;\n  return;\n}","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"The Julia wrapper:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function cdot(x::AbstractVector{Complex{Float64}}, y::AbstractVector{Complex{Float64}})\n    c = Ref{Complex{Float64}}()\n    @ccall OPENMPTEST.cdot(c::Ref{Complex{Float64}}, x::Ref{Complex{Float64}}, y::Ref{Complex{Float64}}, length(x)::Clong)::Cvoid\n    c[]\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"The complex dot product is more compute bound. Given the same number of elements, we require 2x the memory for complex numbers, 4x the floating point arithmetic, and as we have an array of structs rather than structs of arrays, we need additional instructions to shuffle the data. (Image: complexdot)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"If we take this further to the three-argument dot product, which isn't implemented in BLAS, @tturbo now holds a substantial advantage over the competition:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function dot3(x::AbstractVector{Complex{T}}, A::AbstractMatrix{Complex{T}}, y::AbstractVector{Complex{T}}) where {T}\n    xr = reinterpret(reshape, T, x);\n    yr = reinterpret(reshape, T, y);\n    Ar = reinterpret(reshape, T, A);\n    sre = zero(T)\n    sim = zero(T)\n    @tturbo for n in axes(Ar,3)\n        tre = zero(T)\n        tim = zero(T)\n        for m in axes(Ar,2)\n            tre += xr[1,m] * Ar[1,m,n] + xr[2,m] * Ar[2,m,n]\n            tim += xr[1,m] * Ar[2,m,n] - xr[2,m] * Ar[1,m,n]\n        end\n        sre += tre * yr[1,n] - tim * yr[2,n]\n        sim += tre * yr[2,n] + tim * yr[1,n]\n    end\n    Complex(sre, sim)\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"void cdot3(double* c, double* x, double* A, double* y, long M, long N){\n  double sr = 0.0, si = 0.0;\n#pragma omp parallel for reduction(+: sr, si)\n  for (long n = 0; n < N; n++){\n    double tr = 0.0, ti = 0.0;\n    for(long m = 0; m < M; m++){\n      tr += x[2*m] * A[2*m   + 2*n*N] + x[2*m+1] * A[2*m+1 + 2*n*N];\n      ti += x[2*m] * A[2*m+1 + 2*n*N] - x[2*m+1] * A[2*m   + 2*n*N];\n    }\n    sr += tr * y[2*n  ] - ti * y[2*n+1];\n    si += tr * y[2*n+1] + ti * y[2*n  ];\n  }\n  c[0] = sr;\n  c[1] = si;\n  return;\n}","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"The wrapper is more or less the same as before:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function cdot(x::AbstractVector{Complex{Float64}}, A::AbstractMatrix{Complex{Float64}}, y::AbstractVector{Complex{Float64}})\n    c = Ref{Complex{Float64}}()\n    M, N = size(A)\n    @ccall OPENMPTEST.cdot3(c::Ref{Complex{Float64}}, x::Ref{Complex{Float64}}, A::Ref{Complex{Float64}}, y::Ref{Complex{Float64}}, M::Clong, N::Clong)::Cvoid\n    c[]\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"(Image: complexdot3)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"When testing on my laptop, the C implentation ultimately won, but I will need to investigate further to tell whether this benchmark benefits from hyperthreading, or if it's because LoopVectorization's memory access patterns are less friendly. I plan to work on cache-level blocking to increase memory friendliness eventually, and will likely also allow it to take advantage of hyperthreading/simultaneous multithreading, although I'd prefer a few motivating test problems to look at first. Note that a single core of this CPU is capable of exceeding 100 GFLOPS of double precision compute. The execution units are spending most of their time idle. So the question of whether hypthreading helps may be one of whether or not we are memory-limited.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"For a more compute-limited operation, lets look at matrix multiplication, which requires O(NÂ³) compute for O(NÂ²) memory. Note that it's still easy to be memory-starved in matrix multiplication, especially for larger matrices. While the total memory required may be O(NÂ²), if the memory doesn't fit in the high cache levels, it will have to churn through it. The memory bandwidth requirements are thus O(NÂ³), but cache-level blocking can give it a small enough coefficient that you can make the most of your CPU's theoretical compute. Unlike all the dot product cases (including the 3-argument dot product), which force you to stream most of the memory through the cores. There is no reuse on x and y for the 2-arg dot products, or on memory from A in the the 3-arg dot product.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Here, I compare against other libraries: Intel MKL, OpenBLAS (Julia's default), and two Julia libraries: Tullio.jl and Octavian.jl.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"function A_mul_B!(C, A, B)\n    @tturbo for n âˆˆ indices((C,B), 2), m âˆˆ indices((C,A), 1)\n        Cmn = zero(eltype(C))\n        for k âˆˆ indices((A,B), (2,1))\n            Cmn += C[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Benchmarks over the size range 10:5:300: (Image: matmul)","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Because LoopVectorization doesn't do cache optimizations yet, MKL, OpenBLAS, and Octavian will all pull ahead for larger matrices. This CPU has a 1 MiB L2 cache per core and 18 cores:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> doubles_per_l2 = (2 ^ 20) Ã· 8\n131072\n\njulia> total_doubles_in_l2 = doubles_per_l2 * (Sys.CPU_THREADS Ã· 2) # doubles_per_l2 * 18\n2359296\n\njulia> doubles_per_mat = total_doubles_in_l2 Ã· 3 # divide up amoung 3 matrices\n786432\n\njulia> sqrt(ans)\n886.8100134752651","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Meaning we could fit three 886x886 matrices in our L2 cache by splitting them up among the cores. The largest matrices benchmarked above, at 300x300, fit comfortably.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Aside from the fact that LoopVectorization did much better than OpenBLASâ€“Julia's default libraryâ€“over this size range, LoopVectorization's major advantage that it should perform similarly well for a wide variety of comparable operations and not just GEMM (GEneral Matrix-Matrix multiplication) specifically. GEMM has long been a motivating benchmark, as it's one of the best optimized routines available to compare against and get a sense of how well you're doing vs hand-tuned limits optimized in assembly.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Because it is so well optimized, a standard trick for implementing more general optimized routines is to convert them into GEMM calls. For example, this is commonly done for temsor operations (see, e.g., TensorOperations.jl) as well as for convolutions, e.g. in NNlib's conv_im2col!, their default optimized convolution function.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Lets take a look at convolutions as our next example. We create a batch of a hundred 256x256 images with 3 input channels, and convolve them with a 5x5 kernel producing 6 output channels.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"using NNlib, LoopVectorization, Static\n\nimg = rand(Float32, 260, 260, 3, 100);\nkern = rand(Float32, 5, 5, 3, 6);\nout1 = Array{Float32}(undef, size(img,1)+1-size(kern,1), size(img,2)+1-size(kern,2), size(kern,4), size(img,4));\nout2 = similar(out1);\n\ndcd = NNlib.DenseConvDims(img, kern, flipkernel = true);\n\nfunction kernaxes(::DenseConvDims{2,K,C_in, C_out}) where {K,C_in, C_out} # LoopVectorization can take advantage of static size information\n    Kâ‚ =  StaticInt(1):StaticInt(K[1])\n    Kâ‚‚ =  StaticInt(1):StaticInt(K[2])\n    Cáµ¢â‚™ =  StaticInt(1):StaticInt(C_in)\n    Câ‚’áµ¤â‚œ = StaticInt(1):StaticInt(C_out)\n    (Kâ‚, Kâ‚‚, Cáµ¢â‚™, Câ‚’áµ¤â‚œ)\nend\n\nfunction convlayer!(out::AbstractArray{<:Any,4}, img, kern, dcd::DenseConvDims)\n    (Kâ‚, Kâ‚‚, Cáµ¢â‚™, Câ‚’áµ¤â‚œ) = kernaxes(dcd)\n    @tturbo for jâ‚ âˆˆ axes(out,1), jâ‚‚ âˆˆ axes(out,2), d âˆˆ axes(out,4), o âˆˆ Câ‚’áµ¤â‚œ\n        s = zero(eltype(out))\n        for kâ‚ âˆˆ Kâ‚, kâ‚‚ âˆˆ Kâ‚‚, i âˆˆ Cáµ¢â‚™\n            s += img[jâ‚ + kâ‚ - 1, jâ‚‚ + kâ‚‚ - 1, i, d] * kern[kâ‚, kâ‚‚, i, o]\n        end\n        out[jâ‚, jâ‚‚, o, d] = s\n    end\n    out\nend","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization likes to take advantage of any static size information when available, so we write kernaxes to extract them from the DenseConvDims object and produce statically sized axes. Otherwise, this code is simply writing the convolutions as a bunch of loops.","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"This yields:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> NNlib.conv!(out2, img, kern, dcd);\n\njulia> convlayer!(out1, img, kern, dcd);\n\njulia> out1 â‰ˆ out2\ntrue\n\njulia> @benchmark convlayer!($out1, $img, $kern, $dcd)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     5.377 ms (0.00% GC)\n  median time:      5.432 ms (0.00% GC)\n  mean time:        5.433 ms (0.00% GC)\n  maximum time:     5.682 ms (0.00% GC)\n  --------------\n  samples:          920\n  evals/sample:     1\n\njulia> @benchmark NNlib.conv!($out2, $img, $kern, $dcd)\nBenchmarkTools.Trial:\n  memory estimate:  675.02 MiB\n  allocs estimate:  195\n  --------------\n  minimum time:     182.749 ms (0.00% GC)\n  median time:      190.472 ms (0.60% GC)\n  mean time:        197.527 ms (4.98% GC)\n  maximum time:     300.536 ms (35.82% GC)\n  --------------\n  samples:          26\n  evals/sample:     1","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"By default, the BLAS library called uses multiple threads, but NNlib also threads over the batches using Threads.@threads. This oversubscribes the threads. We thus improve performance by forcing BLAS to use just a single thread, favoring the more granular threading across batches:","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"julia> using LinearAlgebra\n\njulia> BLAS.set_num_threads(1)\n\njulia> @benchmark NNlib.conv!($out2, $img, $kern, $dcd)\nBenchmarkTools.Trial:\n  memory estimate:  675.02 MiB\n  allocs estimate:  195\n  --------------\n  minimum time:     124.177 ms (0.00% GC)\n  median time:      128.609 ms (0.93% GC)\n  mean time:        133.574 ms (5.36% GC)\n  maximum time:     235.760 ms (45.17% GC)\n  --------------\n  samples:          38\n  evals/sample:     1","category":"page"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"This still nets @tturbo a 23x advantage on this machine!","category":"page"},{"location":"examples/multithreading/#FAQ","page":"Multithreading","title":"FAQ","text":"","category":"section"},{"location":"examples/multithreading/#If-I-do-@turbo-threadtrue-for-...-end,-how-many-threads-will-it-use?-Or-if-I-do-@turbo-thread4-for-...-end,-what-then?","page":"Multithreading","title":"If I do @turbo thread=true for ... end, how many threads will it use? Or if I do @turbo thread=4 for ... end, what then?","text":"","category":"section"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"LoopVectorization will choose how many threads to use based on the length of the loop ranges and how expensive it estimates evaluating the loop to be. It will at most use one thread per physical core of the system.","category":"page"},{"location":"examples/multithreading/#How-do-I-get-answers-to-my-questions?","page":"Multithreading","title":"How do I get answers to my questions?","text":"","category":"section"},{"location":"examples/multithreading/","page":"Multithreading","title":"Multithreading","text":"Feel free to ask on Discourse, Zulip, Slack, or GitHub Discussions! I can also add it to the FAQ here, or one in an appropriate section.","category":"page"},{"location":"examples/matrix_multiplication/#Matrix-Multiplication","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"One of the friendliest problems for vectorization is matrix multiplication. Given M Ã— K matrix ğ€, and K Ã— N matrix ğ, multiplying them is like performing M * N dot products of length K. We need M*K + K*N + M*N total memory, but M*K*N multiplications and additions, so there's a lot more arithmetic we can do relative to the memory needed.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"LoopVectorization currently doesn't do any memory-modeling or memory-based optimizations, so it will still run into problems as the size of matrices increases. But at smaller sizes, it's capable of achieving a healthy percent of potential GFLOPS. We can write a single function:","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"function A_mul_B!(C, A, B)\n    @turbo for n âˆˆ indices((C,B), 2), m âˆˆ indices((C,A), 1)\n        Cmn = zero(eltype(C))\n        for k âˆˆ indices((A,B), (2,1))\n            Cmn += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\nend","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"and this can handle all transposed/not-tranposed permutations. LoopVectorization will change loop orders and strategy as appropriate based on the types of the input matrices. For each of the others, I wrote separate functions to handle each case.  Letting all three matrices be square and Size x Size, we attain the following benchmark results:","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"(Image: AmulB) This is classic GEMM, ğ‚ = ğ€ * ğ. GFortran's intrinsic matmul function does fairly well. But all the compilers are well behind LoopVectorization here, which falls behind MKL's gemm beyond 70x70 or so. The problem imposed by alignment is also striking: performance is much higher when the sizes are integer multiplies of 8. Padding arrays so that each column is aligned regardless of the number of rows can thus be very profitable. PaddedMatrices.jl offers just such arrays in Julia. I believe that is also what the -pad compiler flag does when using Intel's compilers.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"(Image: AmulBt) The optimal pattern for ğ‚ = ğ€ * ğáµ€ is almost identical to that for ğ‚ = ğ€ * ğ. Yet, gfortran's matmul instrinsic stumbles, surprisingly doing much worse than gfortran + loops, and almost certainly worse than allocating memory for ğáµ€ and creating the ecplicit copy.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"ifort did equally well whethor or not ğ was transposed, while LoopVectorization's performance degraded slightly faster as a function of size in the transposed case, because strides between memory accesses are larger when ğ is transposed. But it still performed best of all the compiled loops over this size range, losing out to MKL and eventually OpenBLAS. icc interestingly does better when it is transposed.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"GEMM is easiest when the matrix ğ€ is not tranposed (assuming column-major memory layouts), because then you can sum up columns of ğ€ to store into ğ‚. If ğ€ were transposed, then we cannot efficiently load contiguous elements from ğ€ that can best stored directly in ğ‚. So for ğ‚ = ğ€áµ€ * ğ, contiguous vectors along the k-loop have to be reduced, adding some overhead. (Image: AtmulB) Packing is critical for performance here. LoopVectorization does not pack, therefore it is well behind MKL and OpenBLAS, which do. Eigen packs, but is poorly optimized for this CPU architecture.","category":"page"},{"location":"examples/matrix_multiplication/","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"When both ğ€ and ğ are transposed, we now have ğ‚ = ğ€áµ€ * ğáµ€ = (ğ * ğ€)áµ€. (Image: AtmulBt) Julia, Clang, and gfortran all struggled to vectorize this, because none of the matrices share a contiguous access: M for ğ‚, K for ğ€áµ€, and N for ğáµ€. However, LoopVectorization and all the specialized matrix multiplication functions managed to do about as well as normal; transposing while storing the results takes negligible amounts of time relative to the matrix multiplication itself. The ifort-loop version also did fairly well.","category":"page"},{"location":"examples/sum_of_squared_error/#Sum-of-squared-error","page":"Sum of squared error","title":"Sum of squared error","text":"","category":"section"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"To calculate (y - X * Î²)'(y - X * Î²), we can use the following loop.","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"function sse_avx(y, X, Î²)\n    lp = zero(eltype(y))\n    @turbo for i âˆˆ eachindex(y)\n        Î´ = y[i]\n        for j âˆˆ eachindex(Î²)\n            Î´ -= X[i,j] * Î²[j]\n        end\n        lp += Î´ * Î´\n    end\n    lp\nend","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"This example demonstrates the importance of (not) modeling memory bandwidth and cache, as the performance quickly drops dramatically. However, it still does much better than all the compiled loops, with only the BLAS gemv-based approach matching (and ultimately beating) it in performance, while the other compilers lagged well behind.","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"Performance starts to degrade for sizes larger than 60. Letting N be the size, X was a 3N/2x N/2 matrix. Therefore, performance started to suffer when X had more than about 30 columns (performance is much less sensitive to the number of rows).","category":"page"},{"location":"examples/sum_of_squared_error/","page":"Sum of squared error","title":"Sum of squared error","text":"(Image: sse)","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To install LoopVectorization.jl, simply use the package and ] add LoopVectorization, or","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"LoopVectorization\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Currently LoopVectorization only supports rectangular iteration spaces, although I plan on extending it to triangular and ragged iteration spaces in the future. This means that if you nest multiple loops, the number of iterations of the inner loops shouldn't be a function of the outer loops. For example,","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using LoopVectorization \n\nfunction mvp(P, basis, coeffs::Vector{T}) where {T}\n    C = length(coeffs)\n    A = size(P, 1)\n    p = zero(T)\n    @turbo for c âˆˆ 1:C\n        pc = coeffs[c]\n        for a = 1:A\n            pc *= P[a, basis[a, c]]\n        end\n        p += pc\n    end\n    p\nend\n\nmaxdeg = 20; nbasis = 1_000; dim = 15;\nr = 1:maxdeg+1\nbasis = rand(r, (dim, nbasis));\ncoeffs = rand(T, nbasis);\nP = rand(T, dim, maxdeg+1);\n\nmvp(P, basis, coeffs)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Aside from loops, LoopVectorization.jl also supports broadcasting.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"danger: Danger\nBroadcasting an Array A when size(A,1) == 1 is NOT SUPPORTED, unless this is known at compile time (e.g., broadcasting a transposed vector is fine). Otherwise, you will probably crash Julia.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> M, K, N = 47, 73, 7;\n\njulia> A = rand(M, K);\n\njulia> b = rand(K);\n\njulia> c = rand(M);\n\njulia> d = rand(1,K,N);\n\njulia> #You can use a LowDimArray when you have a leading dimension of size 1.\n       ldad = LowDimArray{(false,true,true)}(d);\n\njulia> E1 = Array{Float64}(undef, M, K, N);\n\njulia> E2 = similar(E1);\n\njulia> @benchmark @. $E1 = exp($A - $b' +    $d) * $c\nBenchmarkTools.Trial: \n  memory estimate:  112 bytes\n  allocs estimate:  5\n  --------------\n  minimum time:     224.142 Î¼s (0.00% GC)\n  median time:      225.773 Î¼s (0.00% GC)\n  mean time:        229.146 Î¼s (0.00% GC)\n  maximum time:     289.601 Î¼s (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> @benchmark @turbo @. $E2 = exp($A - $b' + $ldad) * $c\nBenchmarkTools.Trial: \n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     19.666 Î¼s (0.00% GC)\n  median time:      19.737 Î¼s (0.00% GC)\n  mean time:        19.759 Î¼s (0.00% GC)\n  maximum time:     29.906 Î¼s (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> E1 â‰ˆ E2\ntrue","category":"page"},{"location":"examples/dot_product/#Dot-Products","page":"Dot Products","title":"Dot Products","text":"","category":"section"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Dot products are simple the sum of the elementwise products of two vectors. They can be interpreted geometrically as (after normalizing by dividing by the norms of both vectors) yielding the cosine of the angle between them. This makes them useful for, for example, the No-U-Turn sampler to check for u-turns (i.e., to check if the current momentum is no longer in the same direction as the change in position).","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"function jdotavx(a, b)\n    s = zero(eltype(a))\n    @turbo for i âˆˆ eachindex(a, b)\n        s += a[i] * b[i]\n    end\n    s\nend","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"To execute the loop using SIMD (Single Instruction Multiple Data) instructions, you have to unroll the loop. Rather than evaluating the loop as written â€“ adding element-wise products to a single accumulator one after the other â€“ you can multiply short vectors loaded from a and b and add their results to a vector of accumulators. ","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Most modern CPUs found in laptops or desktops have the AVX instruction set, which allows them to operate on 256 bit vectors â€“ meaning the vectors can hold 4 double precision (64 bit) floats. Some have the AVX512 instruction set, which increases the vector size to 512 bits, and also adds many new instructions that make vectorizing easier. To be gemeral across CPUs and data types, I'll refer to the number of elements in the vectors with W. I'll also refer to unrolling a loop by a factor of W and loading vectors from it as \"vectorizing\" that loop.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"In addition to vectorizing the loop, we'll want to unroll it by an additional factor. Given that we have single or double precision floating point elements, most recent CPU cores have a potential throughput of two fused multiply-add (fma) instructions per clock cycle. However, it actually takes about four clock cycles for any of these instructions to execute; a single core is able to work on several in parallel.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"This means that if we used a single vector to accumulate a product, we'd only get to perform one fused multiply add every four clock cycles: we'd have to wait for one instruction to complete before starting the next. By using extra accumulation vectors, we can break up this dependency chain. If we had 8 accumulators, then theoretically we could perform two per clock cycle, and after the 4th cycle, our first operations are done so that we can reuse them.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"However, there is another bottle neck: we can only perform 2 aligned loads per clock cycle (or 1 unaligned load). [Alignment here means with respect to a memory address boundary, if your vectors are 256 bits, then a load/store is aligned if it is with respect to a memory address that is an integer multiple of 32 bytes (256 bits = 32 bytes).] Thus, in 4 clock cycles, we can do up to 8 loads. But each fma requires 2 loads, meaning we are limited to 4 of them per 4 clock cyles, and any unrolling beyond 4 gives us no benefit.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Double precision benchmarks pitting Julia's builtin dot product (named MKL here), and code compiled with a variety of compilers: (Image: dot) What we just described is the core of the approach used by all these compilers. The variation in results is explained mostly by how they handle vectors with lengths that are not an integer multiple of W. I ran these on a computer with AVX512 so that W = 8. LLVM, the backend compiler of both Julia and Clang, shows rapid performance degredation as N % 4W increases, where N is the length of the vectors. This is because, to handle the remainder, it uses a scalar loop that runs as written: multiply and add single elements, one after the other. ","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Initially, GCC (gfortran) stumbled in throughput, because it does not use separate accumulation vectors by default except on Power, even with -funroll-loops. I compiled with the flags -fvariable-expansion-in-unroller --param max-variable-expansions-in-unroller=4 to allow for 4 accumulation vectors, yielding good performance.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"The Intel compilers have a secondary vectorized loop without any additional unrolling that masks off excess lanes beyond N (for when N isn't an integer multiple of W). LoopVectorization uses if/ifelse checks to determine how many extra vectors are needed, the last of which is masked.","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Neither GCC nor LLVM use masks (without LoopVectorization's assitance).","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"I am not certain, but I believe Intel and GCC check for the vector's alignment, and align them if neccessary. Julia guarantees that the start of arrays beyond a certain size are aligned, so this is not an optimization I have implemented. But it may be worthwhile for handling large matrices with a number of rows that isn't an integer multiple of W. For such matrices, the first column may be aligned, but the next will not be.","category":"page"},{"location":"examples/dot_product/#Dot-Self","page":"Dot Products","title":"Dot-Self","text":"","category":"section"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"A related problem is taking the dot product of a vector with itself; taking the sum of squares is a common operation, for example when calculating the (log)density of independent normal variates:","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"function jselfdotavx(a)\n    s = zero(eltype(a))\n    @turbo for i âˆˆ eachindex(a)\n        s += a[i] * a[i]\n    end\n    s\nend","category":"page"},{"location":"examples/dot_product/","page":"Dot Products","title":"Dot Products","text":"Because we only need a single load per fma-instruction, we can now benefit from having 8 separate accumulators. For this reason, LoopVectorization now unrolls by 8 â€“ it decides how much to unroll by comparing the bottlenecks on throughput with latency. The other compilers do not change their behavior, so now LoopVectorization has the advantage: (Image: selfdot) This algorithm may need refinement, because Julia (without LoopVectorization) only unrolls by 4, yet achieves roughly the same performance as LoopVectorization at multiples of 4W = 32, although performance declines rapidly from there due to the slow scalar loop. Performance for most is much higher â€“ more GFLOPS â€“ than the normal dot product, but still under half of the CPU's potential 131.2 GFLOPS, suggesting that some other bottlenecks are preventing the core from attaining 2 fmas per clock cycle. Note also that 8W = 64, so we don't really have enough iterations of the loop to amortize the overhead of performing the reductions of all these vectors into a single scalar. By the time the vectors are long enough to do this, we'll start running into memory bandwidth bottlenecks.","category":"page"},{"location":"devdocs/loopset_structure/#LoopSet-Structure","page":"LoopSet Structure","title":"LoopSet Structure","text":"","category":"section"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"The loopsets define loops as a set of operations that depend on one another, and also on loops. Cycles are not allowed, making it a directed acyclic graph. Let's use a set of nested loops performing matrix multiplication as an example. We can create a naive LoopSet from an expression (naive due to being created without access to any type information):","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> using LoopVectorization\n\njulia> AmulBq = :(for m âˆˆ 1:M, n âˆˆ 1:N\n           C[m,n] = zero(eltype(B))\n           for k âˆˆ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end);\n\njulia> lsAmulB = LoopVectorization.LoopSet(AmulBq);","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"This LoopSet consists of seven operations that define the relationships within the loop:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)\n7-element Array{LoopVectorization.Operation,1}:\n var\"##RHS#256\" = var\"##zero#257\"\n C[m, n] = var\"##RHS#256\"\n var\"##tempload#258\" = A[m, k]\n var\"##tempload#259\" = B[k, n]\n var\"##reduction#260\" = var\"##reductzero#261\"\n var\"##reduction#260\" = LoopVectorization.vfmadd_fast(var\"##tempload#258\", var\"##tempload#259\", var\"##reduction#260\")\n var\"##RHS#256\" = LoopVectorization.reduce_to_add(var\"##reduction#260\", var\"##RHS#256\")","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"The act of performing a \"reduction\" across a loop introduces a few extra operations that manage creating a \"zero\" with respect to the reduction, and then combining with the specified value using reduce_to_add, which performs any necessary type conversions, such as from an Vec vector-type to a scalar, if necessary. This simplifies code generation, by making the functions agnostic with respect to the actual vectorization decisions the library makes.","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"Each operation is listed as depending on a set of loop iteration symbols:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.loopdependencies.(LoopVectorization.operations(lsAmulB))\n7-element Array{Array{Symbol,1},1}:\n [:m, :n]\n [:m, :n]\n [:m, :k]\n [:k, :n]\n [:m, :n]\n [:m, :k, :n]\n [:m, :n]","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"We can also see which of the operations each of these operations depend on:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)[6]\nvar\"##reduction#260\" = LoopVectorization.vfmadd_fast(var\"##tempload#258\", var\"##tempload#259\", var\"##reduction#260\")\n\njulia> LoopVectorization.parents(ans)\n3-element Array{LoopVectorization.Operation,1}:\n var\"##tempload#258\" = A[m, k]\n var\"##tempload#259\" = B[k, n]\n var\"##reduction#260\" = var\"##reductzero#261\"","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"References to arrays are represented with an ArrayReferenceMeta data structure:","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)[3].ref\nLoopVectorization.ArrayReferenceMeta(LoopVectorization.ArrayReference(:A, [:m, :k], Int8[0, 0]), Bool[1, 1], Symbol(\"##vptr##_A\"))","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"It contains the name of the parent array (:A), the indicies [:m,:k], and a boolean vector (Bool[1, 1]) indicating whether these indices are loop iterables. Note that the optimizer assumes arrays are column-major, and thus that it is efficient to read contiguous elements from the first index. In lower level terms, it means that high-throughput vmov instructions can be used rather than low-throughput gathers. Similar story for storing elements. When no axis has unit stride, the first given index will be the dummy Symbol(\"##DISCONTIGUOUSSUBARRAY##\").","category":"page"},{"location":"devdocs/loopset_structure/","page":"LoopSet Structure","title":"LoopSet Structure","text":"warning: Warning\nCurrently, only single return values are supported (tuple destructuring is not supported in assignments).","category":"page"},{"location":"examples/filtering/#Image-Filtering","page":"Image Filtering","title":"Image Filtering","text":"","category":"section"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"Here, we convolve a small matrix kern with a larger matrix A, storing the results in out, using Julia's generic Cartesian Indexing:","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"using LoopVectorization, OffsetArrays, Images\nkern = Images.Kernel.gaussian((1, 1), (3, 3))\nA = rand(130,130);\nout = OffsetArray(similar(A, size(A) .- size(kernel) .+ 1), -1 .- kernel.offsets);\nfunction filter2davx!(out::AbstractMatrix, A::AbstractMatrix, kern)\n    @turbo for J in CartesianIndices(out)\n        tmp = zero(eltype(out))\n        for I âˆˆ CartesianIndices(kern)\n            tmp += A[I + J] * kern[I]\n        end\n        out[J] = tmp\n    end\n    out\nend","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"These are effectively four nested loops. For all the benchmarks, kern was 3 by 3, making it too small for vectorizing these loops to be particularly profitable. By vectorizing an outer loop instead, it can benefit from SIMD and also avoid having to do a reduction (horizontal addition) of a vector before storing in out, as the vectors can then be stored directly. (Image: dynamicfilter)","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"LoopVectorization achieved much better performance than all the alternatives, which tried vectorizing the inner loops. By making the compilers aware that the inner loops are too short to be worth vectorizing, we can get them to vectorize an outer loop instead. By defining the size of kern as constant in C and Fortran, and using size parameters in Julia, we can inform the compilers: (Image: staticsizefilter) Now all are doing much better than they were before, although still well shy of the 131.2 GFLOPS theoretical limit for the host CPU cores. While they all improved, two are lagging behind the main group:","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"ifort lags behind all the others except base Julia. I'll need to do more investigating to find out why.\nBase Julia. While providing static size information was enough for it to realize vectorizing the inner loops was not worth it, base Julia was seemingly the only one that didn't decide to vectorize an outer loop instead.","category":"page"},{"location":"examples/filtering/","page":"Image Filtering","title":"Image Filtering","text":"Manually unrolling the inner loops allows base Julia to vectorize, while the performance of all non-Julia variants was unchanged: (Image: unrolledfilter) LoopVectorization is currently limited to only unrolling two loops (but a third may be vectorized, effectively unrolling it by the length of the vectors). Manually unrolling two of the loops lets up to four loops be unrolled.","category":"page"},{"location":"#LoopVectorization.jl","page":"Home","title":"LoopVectorization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This documentation is for LoopVectorization.jl. Please file an issue if you run into any problems.","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"getting_started.md\",\n    \"examples/multithreading.md\",\n    \"examples/matrix_multiplication.md\",\n    \"examples/array_interface.md\",\n    \"examples/matrix_vector_ops.md\",\n    \"examples/dot_product.md\",\n    \"examples/filtering.md\",\n    \"examples/special_functions.md\",\n    \"examples/sum_of_squared_error.md\",\n    \"vectorized_convenience_functions.md\",\n    \"future_work.md\",\n    \"devdocs/overview.md\",\n    \"devdocs/loopset_structure.md\",\n    \"devdocs/constructing_loopsets.md\",\n    \"devdocs/evaluating_loops.md\",\n    \"devdocs/lowering.md\"\n]\nDepth = 1","category":"page"}]
}
